{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7abb6c7",
   "metadata": {},
   "source": [
    "## 7/10: File used for initial testing of running randomization agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa18fbd",
   "metadata": {},
   "source": [
    "This is all some intro code just to visualize some of the settings and get a baseline. You can run through it if you want to get familiar with what the scenario arguments, agents, and training looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50c4a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = 'data.yaml'\n",
    "\n",
    "\n",
    "def writeToYAML():\n",
    "    with open(testfile, 'w') as f:\n",
    "        data = yaml.dump(curr_data, f, sort_keys=False, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faf94d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_dict(dict_obj, indent = 0):\n",
    "    ''' Pretty Print nested dictionary with given indent level  \n",
    "    '''\n",
    "    # Iterate over all key-value pairs of dictionary\n",
    "    for key, value in dict_obj.items():\n",
    "        # If value is dict type, then print nested dict \n",
    "        if isinstance(value, dict):\n",
    "            print(' ' * indent, key, ':', '{')\n",
    "            print_nested_dict(value, indent + 4)\n",
    "            print(' ' * indent, '}')\n",
    "        else:\n",
    "            print(' ' * indent, key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f1ffde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_args={\n",
    "    \"num_hosts\": 5,         # Number of hosts in the network \n",
    "    \n",
    "    \"num_services\": 3,      # Number of services on the network (ssh, ftp, http)\n",
    "    \n",
    "    \"num_os\": 2,            # Number of operatings systems on the network (windows, linux, etc)\n",
    "    \n",
    "    \"num_processes\": 2,     # Number of processes on the network (tomcat, daclsvc, etc)\n",
    "    \n",
    "    \"num_exploits\": None,   # Number of exploits to use\n",
    "    \n",
    "    \"num_privescs\": None,   # Number of privilege escalation actions\n",
    "    \n",
    "    \"r_sensitive\": 10,      # Reward for sensitive subnet documents (default 10)\n",
    "    \n",
    "    \"r_user\": 10,           # Reward for user subnet documents      (default 10)\n",
    "    \n",
    "    \"exploit_cost\": 1,      # Cost to use an exploit (default 1)\n",
    "    \n",
    "    \"exploit_probs\": 1.0,   # Sucess probability of exploits (default 1.0)\n",
    "    \n",
    "    \"privesc_cost\": 1,      # Cost of privilege escalation action (default 1)\n",
    "    \n",
    "    \"privesc_probs\": 1.0,   # Sucess probability of privilege escalation action (default 1.0)\n",
    "    \n",
    "    \"service_scan_cost\": 1, # Cost for a service scan (default 1)\n",
    "    \n",
    "    \"os_scan_cost\": 1,      # Cost for an OS scan (default 1)\n",
    "    \n",
    "    \"subnet_scan_cost\": 1,  # Cost for a subnet scan (default 1)\n",
    "    \n",
    "    \"process_scan_cost\": 1, # Cost for a process scan (default 1)\n",
    "    \n",
    "    \"uniform\": False,       # Whether to use uniform distribution or correlaed host configuration (default false)\n",
    "    \n",
    "    \"alpha_H\": 2.0,         # Scaling or concentration parameter for controlling corelation between host configurations (default 2.0)\n",
    "    \n",
    "    \"alpha_V\": 2.0,         # Scaling or concentration parameter for controlling corelation between services across host configruations (default 2.0)\n",
    "    \n",
    "    \"lambda_V\": 1.0,        # Parameter for controlling average number of services running per host configuration (default 1.0)\n",
    "    \n",
    "    \"restrictiveness\": 5,   # Maximum number of services allowed to pass through firewalls between zones (default 5)\n",
    "    \n",
    "    \"random_goal\": False,   # Whether to randomly assign the goal user host or not (default False)\n",
    "    \n",
    "    \"base_host_value\": 1,   # Value of non sensitive hosts (default 1)\n",
    "    \n",
    "    \"host_discovery_value\": 1,  # Value of discovering a host for the first time (default 1)\n",
    "    \n",
    "    \"seed\": None,           # Random number generator seed (default None)\n",
    "    \n",
    "    \"name\": None,           # Name of the scenario, one will be generated if None (default None)\n",
    "    \n",
    "    \"step_limit\": None}     # Max number of steps permitted in a single episode, None means no limit (default None)\n",
    "\n",
    "#Scenario Generator Parameter List: https://networkattacksimulator.readthedocs.io/en/latest/reference/scenarios/generator.html#scenario-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed762d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario Description: \n",
      "     Name : small\n",
      "     Type : static\n",
      "     Subnets : 5\n",
      "     Hosts : 8\n",
      "     OS : 2\n",
      "     Services : 3\n",
      "     Processes : 2\n",
      "     Exploits : 3\n",
      "     PrivEscs : 2\n",
      "     Actions : 72\n",
      "     Observation Dims : (9, 23)\n",
      "     States : 24576\n",
      "     Step Limit : 1000\n",
      "\n",
      "Scenario Dictionary: \n",
      "       subnets : [1, 1, 1, 5, 1]\n",
      "       topology : [[1, 1, 0, 0, 0], [1, 1, 1, 1, 0], [0, 1, 1, 1, 0], [0, 1, 1, 1, 1], [0, 0, 0, 1, 1]]\n",
      "       os : ['linux', 'windows']\n",
      "       services : ['ssh', 'ftp', 'http']\n",
      "       processes : ['tomcat', 'daclsvc']\n",
      "       sensitive_hosts : {\n",
      "           (2, 0) : 100\n",
      "           (4, 0) : 100\n",
      "       }\n",
      "       exploits : {\n",
      "           e_ssh : {\n",
      "               service : ssh\n",
      "               os : linux\n",
      "               prob : 0.9\n",
      "               cost : 3\n",
      "               access : 1\n",
      "           }\n",
      "           e_ftp : {\n",
      "               service : ftp\n",
      "               os : windows\n",
      "               prob : 0.6\n",
      "               cost : 1\n",
      "               access : 1\n",
      "           }\n",
      "           e_http : {\n",
      "               service : http\n",
      "               os : None\n",
      "               prob : 0.9\n",
      "               cost : 2\n",
      "               access : 1\n",
      "           }\n",
      "       }\n",
      "       privilege_escalation : {\n",
      "           pe_tomcat : {\n",
      "               process : tomcat\n",
      "               os : linux\n",
      "               prob : 1.0\n",
      "               cost : 1\n",
      "               access : 2\n",
      "           }\n",
      "           pe_daclsvc : {\n",
      "               process : daclsvc\n",
      "               os : windows\n",
      "               prob : 1.0\n",
      "               cost : 1\n",
      "               access : 2\n",
      "           }\n",
      "       }\n",
      "       os_scan_cost : 1\n",
      "       service_scan_cost : 1\n",
      "       subnet_scan_cost : 1\n",
      "       process_scan_cost : 1\n",
      "       firewall : {\n",
      "           (0, 1) : ['http']\n",
      "           (1, 0) : []\n",
      "           (1, 2) : ['ssh']\n",
      "           (2, 1) : ['ssh']\n",
      "           (1, 3) : []\n",
      "           (3, 1) : ['ssh']\n",
      "           (2, 3) : ['http']\n",
      "           (3, 2) : ['ftp']\n",
      "           (3, 4) : ['ssh', 'ftp']\n",
      "           (4, 3) : ['ftp']\n",
      "       }\n",
      "       host : {\n",
      "           (1, 0) : Host: {\n",
      "\taddress: (1, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t\twindows: False\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: False\n",
      "\t\thttp: True\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (2, 0) : Host: {\n",
      "\taddress: (2, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 100.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t\twindows: False\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: True\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: True\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 0) : Host: {\n",
      "\taddress: (3, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 1) : Host: {\n",
      "\taddress: (3, 1)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: True\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 2) : Host: {\n",
      "\taddress: (3, 2)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 3) : Host: {\n",
      "\taddress: (3, 3)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 4) : Host: {\n",
      "\taddress: (3, 4)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (4, 0) : Host: {\n",
      "\taddress: (4, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 100.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t\twindows: False\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: True\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: True\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "       }\n",
      "       step_limit : 1000\n"
     ]
    }
   ],
   "source": [
    "import nasim\n",
    "import json\n",
    "env = nasim.generate(**scenario_args)\n",
    "env = nasim.make_benchmark(\"huge-gen\")\n",
    "env = nasim.load(\"unreachable.yaml\")\n",
    "env2 = env = nasim.make_benchmark(\"small\")\n",
    "\n",
    "\n",
    "scenario_desc = env.scenario.get_description() #get_description found in scenario.py file under nasim->scenarios\n",
    "scenario_dict = env.scenario.scenario_dict\n",
    "#scenario_exploit_map = env.scenario.exploit_map # A nested dictionary for all exploits in scenario.\n",
    "#scenario_privesc_map = env.scenario.privesc_map # A nested dictionary for all privilege escalation actions in scenario.\n",
    "\n",
    "print(\"Scenario Description: \")\n",
    "print_nested_dict(scenario_desc,4)\n",
    "\n",
    "print(\"\\nScenario Dictionary: \")\n",
    "print_nested_dict(scenario_dict,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15cbdb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.get_minimum_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ac9a3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--render_eval] [--lr LR]\n",
      "                             [-t TRAINING_STEPS] [--batch_size BATCH_SIZE]\n",
      "                             [--seed SEED] [--replay_size REPLAY_SIZE]\n",
      "                             [--final_epsilon FINAL_EPSILON]\n",
      "                             [--init_epsilon INIT_EPSILON]\n",
      "                             [-e EXPLORATION_STEPS] [--gamma GAMMA] [--quite]\n",
      "                             env_name\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/nasim/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3450: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    " \"\"\"An example Tabular, epsilon greedy Q-Learning Agent.\n",
    "\n",
    "This agent does not use an Experience replay (see the 'ql_replay_agent.py')\n",
    "\n",
    "It uses pytorch 1.5+ tensorboard library for logging (HINT: these dependencies\n",
    "can be installed by running pip install nasim[dqn])\n",
    "\n",
    "To run 'tiny' benchmark scenario with default settings, run the following from\n",
    "the nasim/agents dir:\n",
    "\n",
    "$ python ql_agent.py tiny\n",
    "\n",
    "To see detailed results using tensorboard:\n",
    "\n",
    "$ tensorboard --logdir runs/\n",
    "\n",
    "To see available hyperparameters:\n",
    "\n",
    "$ python ql_agent.py --help\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "This is by no means a state of the art implementation of Tabular Q-Learning.\n",
    "It is designed to be an example implementation that can be used as a reference\n",
    "for building your own agents and for simple experimental comparisons.\n",
    "\"\"\"\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import nasim\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError as e:\n",
    "    from gymnasium import error\n",
    "    raise error.DependencyNotInstalled(\n",
    "        f\"{e}. (HINT: you can install tabular_q_learning_agent dependencies \"\n",
    "        \"by running 'pip install nasim[dqn]'.)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class TabularQFunction:\n",
    "    \"\"\"Tabular Q-Function \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        self.q_func = dict()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = str(x.astype(int))\n",
    "        if x not in self.q_func:\n",
    "            self.q_func[x] = np.zeros(self.num_actions, dtype=np.float32)\n",
    "        return self.q_func[x]\n",
    "\n",
    "    def forward_batch(self, x_batch):\n",
    "        return np.asarray([self.forward(x) for x in x_batch])\n",
    "\n",
    "    def update_batch(self, s_batch, a_batch, delta_batch):\n",
    "        for s, a, delta in zip(s_batch, a_batch, delta_batch):\n",
    "            q_vals = self.forward(s)\n",
    "            q_vals[a] += delta\n",
    "\n",
    "    def update(self, s, a, delta):\n",
    "        q_vals = self.forward(s)\n",
    "        q_vals[a] += delta\n",
    "\n",
    "    def get_action(self, x):\n",
    "        return int(self.forward(x).argmax())\n",
    "\n",
    "    def display(self):\n",
    "        pprint(self.q_func)\n",
    "\n",
    "\n",
    "class TabularQLearningAgent:\n",
    "    \"\"\"A Tabular. epsilon greedy Q-Learning Agent using Experience Replay \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 seed=None,\n",
    "                 lr=0.001,\n",
    "                 training_steps=10000,\n",
    "                 final_epsilon=0.05,\n",
    "                 exploration_steps=10000,\n",
    "                 gamma=0.99,\n",
    "                 verbose=True,\n",
    "                 **kwargs):\n",
    "\n",
    "        # This implementation only works for flat actions\n",
    "        assert env.flat_actions\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(\"\\nRunning Tabular Q-Learning with config:\")\n",
    "            pprint(locals())\n",
    "\n",
    "        # set seeds\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # envirnment setup\n",
    "        self.env = env\n",
    "\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "\n",
    "        # logger setup\n",
    "        self.logger = SummaryWriter()\n",
    "\n",
    "        # Training related attributes\n",
    "        self.lr = lr\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.epsilon_schedule = np.linspace(\n",
    "            1.0, self.final_epsilon, self.exploration_steps\n",
    "        )\n",
    "        self.discount = gamma\n",
    "        self.training_steps = training_steps\n",
    "        self.steps_done = 0\n",
    "\n",
    "        # Q-Function\n",
    "        self.qfunc = TabularQFunction(self.num_actions)\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        if self.steps_done < self.exploration_steps:\n",
    "            return self.epsilon_schedule[self.steps_done]\n",
    "        return self.final_epsilon\n",
    "\n",
    "    def get_egreedy_action(self, o, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            return self.qfunc.get_action(o)\n",
    "        return random.randint(0, self.num_actions-1)\n",
    "\n",
    "    def optimize(self, s, a, next_s, r, done):\n",
    "        # get q_val for state and action performed in that state\n",
    "        q_vals_raw = self.qfunc.forward(s)\n",
    "        q_val = q_vals_raw[a]\n",
    "\n",
    "        # get target q val = max val of next state\n",
    "        target_q_val = self.qfunc.forward(next_s).max()\n",
    "        target = r + self.discount * (1-done) * target_q_val\n",
    "\n",
    "        # calculate error and update\n",
    "        td_error = target - q_val\n",
    "        td_delta = self.lr * td_error\n",
    "\n",
    "        # optimize the model\n",
    "        self.qfunc.update(s, a, td_delta)\n",
    "\n",
    "        s_value = q_vals_raw.max()\n",
    "        return td_error, s_value\n",
    "\n",
    "    def train(self):\n",
    "        if self.verbose:\n",
    "            print(\"\\nStarting training\")\n",
    "\n",
    "        num_episodes = 0\n",
    "        training_steps_remaining = self.training_steps\n",
    "        \n",
    "        ################################################ Lists used for graphing\n",
    "        \n",
    "        ep_return_list = []\n",
    "        ep_steps_list = []\n",
    "        ep_action_list = []\n",
    "        \n",
    "        eval_perf_list = []\n",
    "        eval_step_list = []\n",
    "        eval_action_list = []\n",
    "        eval_info_list = []\n",
    "        \n",
    "        ################################################\n",
    "\n",
    "        while self.steps_done < self.training_steps:\n",
    "            ep_results = self.run_train_episode(training_steps_remaining)\n",
    "            ep_return, ep_steps, ep_actions, goal = ep_results             #ep_actions added\n",
    "            \n",
    "        ################################################ Appending to lists used for graphing\n",
    "        \n",
    "            ep_return_list.append(ep_return)\n",
    "            ep_steps_list.append(ep_steps)\n",
    "            ep_action_list.append(ep_actions)\n",
    "            \n",
    "            eval_perfs, eval_steps, eval_actions, eval_infos = self.run_eval_episode()\n",
    "            \n",
    "            eval_perf_list.append(eval_perfs)\n",
    "            eval_step_list.append(eval_steps)\n",
    "            eval_action_list.append(eval_actions)\n",
    "            eval_info_list.append(eval_infos)\n",
    "        \n",
    "        ################################################\n",
    "            \n",
    "            \n",
    "            \n",
    "            num_episodes += 1\n",
    "            training_steps_remaining -= ep_steps\n",
    "\n",
    "            self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "            self.logger.add_scalar(\n",
    "                \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_return\", ep_return, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_steps\", ep_steps, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_goal_reached\", int(goal), self.steps_done\n",
    "            )\n",
    "\n",
    "            if num_episodes % 10 == 0 and self.verbose:\n",
    "                print(f\"\\nEpisode {num_episodes}:\")\n",
    "                print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                      f\"{self.training_steps}\")\n",
    "                print(f\"\\treturn = {ep_return}\")\n",
    "                print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "        self.logger.close()\n",
    "        if self.verbose:\n",
    "            print(\"Training complete\")\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    def run_train_episode(self, step_limit):\n",
    "        s, _ = self.env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        while not done and not env_step_limit_reached and steps < step_limit:\n",
    "            a = self.get_egreedy_action(s, self.get_epsilon())\n",
    "\n",
    "            next_s, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.steps_done += 1\n",
    "            td_error, s_value = self.optimize(s, a, next_s, r, done)\n",
    "            self.logger.add_scalar(\"td_error\", td_error, self.steps_done)\n",
    "            self.logger.add_scalar(\"s_value\", s_value, self.steps_done)\n",
    "\n",
    "            s = next_s\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "    def run_eval_episode(self,\n",
    "                         env=None,\n",
    "                         render=False,\n",
    "                         eval_epsilon=0.05,\n",
    "                         render_mode=\"human\"):\n",
    "        if env is None:\n",
    "            env = self.env\n",
    "\n",
    "        original_render_mode = env.render_mode\n",
    "        env.render_mode = render_mode\n",
    "\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        line_break = \"=\"*60\n",
    "        if render:\n",
    "            print(\"\\n\" + line_break)\n",
    "            print(f\"Running EVALUATION using epsilon = {eval_epsilon:.4f}\")\n",
    "            print(line_break)\n",
    "            env.render()\n",
    "            input(\"Initial state. Press enter to continue..\")\n",
    "\n",
    "        while not done and not env_step_limit_reached:\n",
    "            a = self.get_egreedy_action(s, eval_epsilon)\n",
    "            next_s, r, done, env_step_limit_reached, _ = env.step(a)\n",
    "            s = next_s\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "            if render:\n",
    "                print(\"\\n\" + line_break)\n",
    "                print(f\"Step {steps}\")\n",
    "                print(line_break)\n",
    "                print(f\"Action Performed = {env.action_space.get_action(a)}\")\n",
    "                env.render()\n",
    "                print(f\"Reward = {r}\")\n",
    "                print(f\"Done = {done}\")\n",
    "                print(f\"Step limit reached = {env_step_limit_reached}\")\n",
    "                input(\"Press enter to continue..\")\n",
    "\n",
    "                if done or env_step_limit_reached:\n",
    "                    print(\"\\n\" + line_break)\n",
    "                    print(\"EPISODE FINISHED\")\n",
    "                    print(line_break)\n",
    "                    print(f\"Goal reached = {env.goal_reached()}\")\n",
    "                    print(f\"Total steps = {steps}\")\n",
    "                    print(f\"Total reward = {episode_return}\")\n",
    "\n",
    "        env.render_mode = original_render_mode\n",
    "        return episode_return, steps, env.goal_reached()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"env_name\", type=str, help=\"benchmark scenario name\")\n",
    "    parser.add_argument(\"--render_eval\", action=\"store_true\",\n",
    "                        help=\"Renders final policy\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001,\n",
    "                        help=\"Learning rate (default=0.001)\")\n",
    "    parser.add_argument(\"-t\", \"--training_steps\", type=int, default=10000,\n",
    "                        help=\"training steps (default=10000)\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                        help=\"(default=32)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0,\n",
    "                        help=\"(default=0)\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=100000,\n",
    "                        help=\"(default=100000)\")\n",
    "    parser.add_argument(\"--final_epsilon\", type=float, default=0.05,\n",
    "                        help=\"(default=0.05)\")\n",
    "    parser.add_argument(\"--init_epsilon\", type=float, default=1.0,\n",
    "                        help=\"(default=1.0)\")\n",
    "    parser.add_argument(\"-e\", \"--exploration_steps\", type=int, default=10000,\n",
    "                        help=\"(default=10000)\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "                        help=\"(default=0.99)\")\n",
    "    parser.add_argument(\"--quite\", action=\"store_false\",\n",
    "                        help=\"Run in Quite mode\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    env = nasim.make_benchmark(\n",
    "        args.env_name,\n",
    "        args.seed,\n",
    "        fully_obs=True,\n",
    "        flat_actions=True,\n",
    "        flat_obs=True\n",
    "    )\n",
    "    ql_agent = TabularQLearningAgent(\n",
    "        env, verbose=args.quite, **vars(args)\n",
    "    )\n",
    "    #ql_agent.train()\n",
    "    #ql_agent.run_eval_episode(render=args.render_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f70ce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Tabular Q-Learning with config:\n",
      "{'env': <nasim.envs.environment.NASimEnv object at 0x7fed46de3b50>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'seed': None,\n",
      " 'self': <__main__.TabularQLearningAgent object at 0x7fec7a77f520>,\n",
      " 'training_steps': 20000,\n",
      " 'verbose': True}\n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#NOW USING AGENT ABOVE INSTEAD OF IMPORTING AGENT:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#USED TO BE: from nasim.agents.ql_agent import TabularQLearningAgent\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ql_agent \u001b[38;5;241m=\u001b[39m TabularQLearningAgent(env2, training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m outputs_from_training \u001b[38;5;241m=\u001b[39m \u001b[43mql_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 180\u001b[0m, in \u001b[0;36mTabularQLearningAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_steps:\n\u001b[1;32m    179\u001b[0m     ep_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_train_episode(training_steps_remaining)\n\u001b[0;32m--> 180\u001b[0m     ep_return, ep_steps, ep_actions, goal \u001b[38;5;241m=\u001b[39m ep_results             \u001b[38;5;66;03m#ep_actions added\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m################################################ Appending to lists used for graphing\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     ep_return_list\u001b[38;5;241m.\u001b[39mappend(ep_return)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "#NOW USING AGENT ABOVE INSTEAD OF IMPORTING AGENT:\n",
    "#USED TO BE: from nasim.agents.ql_agent import TabularQLearningAgent\n",
    "\n",
    "ql_agent = TabularQLearningAgent(env2, training_steps=20000)\n",
    "outputs_from_training = ql_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "206bea9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ep_return_list \u001b[38;5;241m=\u001b[39m \u001b[43moutputs_from_training\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      2\u001b[0m ep_steps_list \u001b[38;5;241m=\u001b[39m outputs_from_training[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      3\u001b[0m ep_action_list \u001b[38;5;241m=\u001b[39m outputs_from_training[\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "ep_return_list = outputs_from_training[0]\n",
    "ep_steps_list = outputs_from_training[1]\n",
    "ep_action_list = outputs_from_training[2]\n",
    "eval_perf_list = outputs_from_training[3]\n",
    "eval_step_list = outputs_from_training[4]\n",
    "eval_action_list = outputs_from_training[5]\n",
    "eval_info_list = outputs_from_training[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17413f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollavg_cumsum(a,n):\n",
    "    assert n%2==1 \n",
    "    cumsum_vec = np.cumsum(np.insert(a, 0, 0)) \n",
    "        #inserts zero at beginning of array to include the 0th element in the array\n",
    "        #cumsum calculates the cumlative sum of the modified array\n",
    "    return (cumsum_vec[n:] - cumsum_vec[:-n]) / n\n",
    "        #calculates the difference between consecutive elements of the array\n",
    "        #this difference is offset by n and divided by n to get the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b51a057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_rewards(ep_return_list, eval_perf_list, max_ep):\n",
    "    print(len(ep_return_list))\n",
    "    ep_return_rolling = rollavg_cumsum(ep_return_list, 5)\n",
    "    eval_return_rolling = rollavg_cumsum(eval_perf_list, 15)\n",
    "    print(len(ep_return_rolling))\n",
    "    plt.scatter(np.arange(len(ep_return_rolling)), ep_return_rolling, c='red', s=1, alpha=0.5)\n",
    "    plt.plot(np.arange(len(eval_return_rolling)), eval_return_rolling, c='blue')\n",
    "    plt.xlim([0, max_ep])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908e3f00",
   "metadata": {},
   "source": [
    "## Current Code \n",
    "Here is the main code to test/run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "475fe954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial scenario arguments... we will be editing the number of hosts by marking actions involving them as invalid\n",
    "scenario_args={\n",
    "    \"num_hosts\": 5,         # Number of hosts in the network \n",
    "    \n",
    "    \"num_services\": 3,      # Number of services on the network (ssh, ftp, http)\n",
    "    \n",
    "    \"num_os\": 2,            # Number of operatings systems on the network (windows, linux, etc)\n",
    "    \n",
    "    \"num_processes\": 2,     # Number of processes on the network (tomcat, daclsvc, etc)\n",
    "    \n",
    "    \"num_exploits\": None,   # Number of exploits to use\n",
    "    \n",
    "    \"num_privescs\": None,   # Number of privilege escalation actions\n",
    "    \n",
    "    \"r_sensitive\": 10,      # Reward for sensitive subnet documents (default 10)\n",
    "    \n",
    "    \"r_user\": 10,           # Reward for user subnet documents      (default 10)\n",
    "    \n",
    "    \"exploit_cost\": 1,      # Cost to use an exploit (default 1)\n",
    "    \n",
    "    \"exploit_probs\": 1.0,   # Sucess probability of exploits (default 1.0)\n",
    "    \n",
    "    \"privesc_cost\": 1,      # Cost of privilege escalation action (default 1)\n",
    "    \n",
    "    \"privesc_probs\": 1.0,   # Sucess probability of privilege escalation action (default 1.0)\n",
    "    \n",
    "    \"service_scan_cost\": 1, # Cost for a service scan (default 1)\n",
    "    \n",
    "    \"os_scan_cost\": 1,      # Cost for an OS scan (default 1)\n",
    "    \n",
    "    \"subnet_scan_cost\": 1,  # Cost for a subnet scan (default 1)\n",
    "    \n",
    "    \"process_scan_cost\": 1, # Cost for a process scan (default 1)\n",
    "    \n",
    "    \"uniform\": False,       # Whether to use uniform distribution or correlaed host configuration (default false)\n",
    "    \n",
    "    \"alpha_H\": 2.0,         # Scaling or concentration parameter for controlling corelation between host configurations (default 2.0)\n",
    "    \n",
    "    \"alpha_V\": 2.0,         # Scaling or concentration parameter for controlling corelation between services across host configruations (default 2.0)\n",
    "    \n",
    "    \"lambda_V\": 1.0,        # Parameter for controlling average number of services running per host configuration (default 1.0)\n",
    "    \n",
    "    \"restrictiveness\": 5,   # Maximum number of services allowed to pass through firewalls between zones (default 5)\n",
    "    \n",
    "    \"random_goal\": False,   # Whether to randomly assign the goal user host or not (default False)\n",
    "    \n",
    "    \"base_host_value\": 1,   # Value of non sensitive hosts (default 1)\n",
    "    \n",
    "    \"host_discovery_value\": 1,  # Value of discovering a host for the first time (default 1)\n",
    "    \n",
    "    \"seed\": None,           # Random number generator seed (default None)\n",
    "    \n",
    "    \"name\": None,           # Name of the scenario, one will be generated if None (default None)\n",
    "    \n",
    "    \"step_limit\": None}     # Max number of steps permitted in a single episode, None means no limit (default None)\n",
    "\n",
    "#Scenario Generator Parameter List: https://networkattacksimulator.readthedocs.io/en/latest/reference/scenarios/generator.html#scenario-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d42474b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Python user-defined exceptions\n",
    "class SensitiveHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (sensitive host needs to remain in network)\"\n",
    "    pass\n",
    "\n",
    "class PublicHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (public host to enter the network... specific to this configuration)\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0822e035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--render_eval] [-o]\n",
      "                             [--hidden_sizes [HIDDEN_SIZES ...]] [--lr LR]\n",
      "                             [-t TRAINING_STEPS] [--batch_size BATCH_SIZE]\n",
      "                             [--target_update_freq TARGET_UPDATE_FREQ]\n",
      "                             [--seed SEED] [--replay_size REPLAY_SIZE]\n",
      "                             [--final_epsilon FINAL_EPSILON]\n",
      "                             [--init_epsilon INIT_EPSILON]\n",
      "                             [--exploration_steps EXPLORATION_STEPS]\n",
      "                             [--gamma GAMMA] [--quite]\n",
      "                             env_name\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT DQN AGENT AS CONTROL\n",
    "\n",
    "\"\"\"An example DQN Agent.\n",
    "\n",
    "It uses pytorch 1.5+ and tensorboard libraries (HINT: these dependencies can\n",
    "be installed by running pip install nasim[dqn])\n",
    "\n",
    "To run 'tiny' benchmark scenario with default settings, run the following from\n",
    "the nasim/agents dir:\n",
    "\n",
    "$ python dqn_agent.py tiny\n",
    "\n",
    "To see detailed results using tensorboard:\n",
    "\n",
    "$ tensorboard --logdir runs/\n",
    "\n",
    "To see available hyperparameters:\n",
    "\n",
    "$ python dqn_agent.py --help\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "This is by no means a state of the art implementation of DQN, but is designed\n",
    "to be an example implementation that can be used as a reference for building\n",
    "your own agents.\n",
    "\"\"\"\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "from gymnasium import error\n",
    "import numpy as np\n",
    "\n",
    "import nasim\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\n",
    "        f\"{e}. (HINT: you can install dqn_agent dependencies by running \"\n",
    "        \"'pip install nasim[dqn]'.)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity, s_dims, device=\"cpu\"):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "        self.a_buf = np.zeros((capacity, 1), dtype=np.int64)\n",
    "        self.next_s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "        self.r_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.ptr, self.size = 0, 0\n",
    "\n",
    "    def store(self, s, a, next_s, r, done):\n",
    "        self.s_buf[self.ptr] = s\n",
    "        self.a_buf[self.ptr] = a\n",
    "        self.next_s_buf[self.ptr] = next_s\n",
    "        self.r_buf[self.ptr] = r\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size+1, self.capacity)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        sample_idxs = np.random.choice(self.size, batch_size)\n",
    "        batch = [self.s_buf[sample_idxs],\n",
    "                 self.a_buf[sample_idxs],\n",
    "                 self.next_s_buf[sample_idxs],\n",
    "                 self.r_buf[sample_idxs],\n",
    "                 self.done_buf[sample_idxs]]\n",
    "        return [torch.from_numpy(buf).to(self.device) for buf in batch]\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"A simple Deep Q-Network \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, layers, num_actions):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_dim[0], layers[0])])\n",
    "        for l in range(1, len(layers)):\n",
    "            self.layers.append(nn.Linear(layers[l-1], layers[l]))\n",
    "        self.out = nn.Linear(layers[-1], num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def save_DQN(self, file_path):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load_DQN(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def get_action(self, x):\n",
    "        with torch.no_grad():\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.view(1, -1)\n",
    "            return self.forward(x).max(1)[1]\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"A simple Deep Q-Network Agent \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 seed=None,\n",
    "                 lr=0.001,\n",
    "                 training_steps=20000,\n",
    "                 batch_size=32,\n",
    "                 replay_size=10000,\n",
    "                 final_epsilon=0.05,\n",
    "                 exploration_steps=10000,\n",
    "                 gamma=0.99,\n",
    "                 hidden_sizes=[64, 64],\n",
    "                 target_update_freq=1000,\n",
    "                 verbose=True,\n",
    "                 **kwargs):\n",
    "\n",
    "        # This DQN implementation only works for flat actions\n",
    "        assert env.flat_actions\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(f\"\\nRunning DQN with config:\")\n",
    "            pprint(locals())\n",
    "\n",
    "        # set seeds\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # environment setup\n",
    "        self.env = env\n",
    "\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "\n",
    "        # logger setup\n",
    "        self.logger = SummaryWriter()\n",
    "\n",
    "        # Training related attributes\n",
    "        self.lr = lr\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.epsilon_schedule = np.linspace(1.0,\n",
    "                                            self.final_epsilon,\n",
    "                                            self.exploration_steps)\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = gamma\n",
    "        self.training_steps = training_steps\n",
    "        self.steps_done = 0\n",
    "\n",
    "        # Neural Network related attributes\n",
    "        self.device = torch.device(\"cuda\"\n",
    "                                   if torch.cuda.is_available()\n",
    "                                   else \"cpu\")\n",
    "        self.dqn = DQN(self.obs_dim,\n",
    "                       hidden_sizes,\n",
    "                       self.num_actions).to(self.device)\n",
    "        if self.verbose:\n",
    "            print(f\"\\nUsing Neural Network running on device={self.device}:\")\n",
    "            print(self.dqn)\n",
    "\n",
    "        self.target_dqn = DQN(self.obs_dim,\n",
    "                              hidden_sizes,\n",
    "                              self.num_actions).to(self.device)\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=self.lr)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # replay setup\n",
    "        self.replay = ReplayMemory(replay_size,\n",
    "                                   self.obs_dim,\n",
    "                                   self.device)\n",
    "\n",
    "    def save(self, save_path):\n",
    "        self.dqn.save_DQN(save_path)\n",
    "\n",
    "    def load(self, load_path):\n",
    "        self.dqn.load_DQN(load_path)\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        if self.steps_done < self.exploration_steps:\n",
    "            return self.epsilon_schedule[self.steps_done]\n",
    "        return self.final_epsilon\n",
    "\n",
    "    def get_egreedy_action(self, o, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            o = torch.from_numpy(o).float().to(self.device)\n",
    "            return self.dqn.get_action(o).cpu().item()\n",
    "        return random.randint(0, self.num_actions-1)\n",
    "\n",
    "    def optimize(self):\n",
    "        batch = self.replay.sample_batch(self.batch_size)\n",
    "        s_batch, a_batch, next_s_batch, r_batch, d_batch = batch\n",
    "\n",
    "        # get q_vals for each state and the action performed in that state\n",
    "        q_vals_raw = self.dqn(s_batch)\n",
    "        q_vals = q_vals_raw.gather(1, a_batch).squeeze()\n",
    "\n",
    "        # get target q val = max val of next state\n",
    "        with torch.no_grad():\n",
    "            target_q_val_raw = self.target_dqn(next_s_batch)\n",
    "            target_q_val = target_q_val_raw.max(1)[0]\n",
    "            target = r_batch + self.discount*(1-d_batch)*target_q_val\n",
    "\n",
    "        # calculate loss\n",
    "        loss = self.loss_fn(q_vals, target)\n",
    "\n",
    "        # optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "        q_vals_max = q_vals_raw.max(1)[0]\n",
    "        mean_v = q_vals_max.mean().item()\n",
    "        return loss.item(), mean_v\n",
    "\n",
    "    def train(self):\n",
    "        if self.verbose:\n",
    "            print(\"\\nStarting training\")\n",
    "\n",
    "        num_episodes = 0\n",
    "        training_steps_remaining = self.training_steps\n",
    "\n",
    "        while self.steps_done < self.training_steps:\n",
    "            ep_results = self.run_train_episode(training_steps_remaining)\n",
    "            ep_return, ep_steps, goal = ep_results #ep_return, ep_steps, and goal equal ep_results\n",
    "            num_episodes += 1\n",
    "            training_steps_remaining -= ep_steps\n",
    "\n",
    "            self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "            self.logger.add_scalar(\n",
    "                \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_return\", ep_return, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_steps\", ep_steps, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_goal_reached\", int(goal), self.steps_done\n",
    "            )\n",
    "\n",
    "            if num_episodes % 10 == 0 and self.verbose:\n",
    "                print(f\"\\nEpisode {num_episodes}:\")\n",
    "                print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                      f\"{self.training_steps}\")\n",
    "                print(f\"\\treturn = {ep_return}\")\n",
    "                print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "        self.logger.close()\n",
    "        if self.verbose:\n",
    "            print(\"Training complete\")\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    def run_train_episode(self, step_limit):\n",
    "        o, _ = self.env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        while not done and not env_step_limit_reached and steps < step_limit:\n",
    "            a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "\n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            self.logger.add_scalar(\"loss\", loss, self.steps_done)\n",
    "            self.logger.add_scalar(\"mean_v\", mean_v, self.steps_done)\n",
    "\n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "    def run_eval_episode(self,\n",
    "                         env=None,\n",
    "                         render=False,\n",
    "                         eval_epsilon=0.05,\n",
    "                         render_mode=\"human\"):\n",
    "        if env is None:\n",
    "            env = self.env\n",
    "\n",
    "        original_render_mode = env.render_mode\n",
    "        env.render_mode = render_mode\n",
    "\n",
    "        o, _ = env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        line_break = \"=\"*60\n",
    "        if render:\n",
    "            print(\"\\n\" + line_break)\n",
    "            print(f\"Running EVALUATION using epsilon = {eval_epsilon:.4f}\")\n",
    "            print(line_break)\n",
    "            env.render()\n",
    "            input(\"Initial state. Press enter to continue..\")\n",
    "\n",
    "        while not done and not env_step_limit_reached:\n",
    "            a = self.get_egreedy_action(o, eval_epsilon)\n",
    "            next_o, r, done, env_step_limit_reached, _ = env.step(a)\n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "            if render:\n",
    "                print(\"\\n\" + line_break)\n",
    "                print(f\"Step {steps}\")\n",
    "                print(line_break)\n",
    "                print(f\"Action Performed = {env.action_space.get_action(a)}\")\n",
    "                env.render()\n",
    "                print(f\"Reward = {r}\")\n",
    "                print(f\"Done = {done}\")\n",
    "                print(f\"Step limit reached = {env_step_limit_reached}\")\n",
    "                input(\"Press enter to continue..\")\n",
    "\n",
    "                if done or env_step_limit_reached:\n",
    "                    print(\"\\n\" + line_break)\n",
    "                    print(\"EPISODE FINISHED\")\n",
    "                    print(line_break)\n",
    "                    print(f\"Goal reached = {env.goal_reached()}\")\n",
    "                    print(f\"Total steps = {steps}\")\n",
    "                    print(f\"Total reward = {episode_return}\")\n",
    "\n",
    "        env.render_mode = original_render_mode\n",
    "        return episode_return, steps, env.goal_reached()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"env_name\", type=str, help=\"benchmark scenario name\")\n",
    "    parser.add_argument(\"--render_eval\", action=\"store_true\",\n",
    "                        help=\"Renders final policy\")\n",
    "    parser.add_argument(\"-o\", \"--partially_obs\", action=\"store_true\",\n",
    "                        help=\"Partially Observable Mode\")\n",
    "    parser.add_argument(\"--hidden_sizes\", type=int, nargs=\"*\",\n",
    "                        default=[64, 64],\n",
    "                        help=\"(default=[64. 64])\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001,\n",
    "                        help=\"Learning rate (default=0.001)\")\n",
    "    parser.add_argument(\"-t\", \"--training_steps\", type=int, default=20000,\n",
    "                        help=\"training steps (default=20000)\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                        help=\"(default=32)\")\n",
    "    parser.add_argument(\"--target_update_freq\", type=int, default=1000,\n",
    "                        help=\"(default=1000)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0,\n",
    "                        help=\"(default=0)\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=100000,\n",
    "                        help=\"(default=100000)\")\n",
    "    parser.add_argument(\"--final_epsilon\", type=float, default=0.05,\n",
    "                        help=\"(default=0.05)\")\n",
    "    parser.add_argument(\"--init_epsilon\", type=float, default=1.0,\n",
    "                        help=\"(default=1.0)\")\n",
    "    parser.add_argument(\"--exploration_steps\", type=int, default=10000,\n",
    "                        help=\"(default=10000)\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "                        help=\"(default=0.99)\")\n",
    "    parser.add_argument(\"--quite\", action=\"store_false\",\n",
    "                        help=\"Run in Quite mode\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    env = nasim.make_benchmark(args.env_name,\n",
    "                               args.seed,\n",
    "                               fully_obs=not args.partially_obs,\n",
    "                               flat_actions=True,\n",
    "                               flat_obs=True)\n",
    "    dqn_agent = DQNAgent(env, verbose=args.quite, **vars(args))\n",
    "    dqn_agent.train()\n",
    "    dqn_agent.run_eval_episode(render=args.render_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb9e544c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2311256159.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    baseline_dqn_agent = DQNAgent(env, verbose=1, training_steps=, max_episodes=1000000) #10-20k steps, 10k episodes\u001b[0m\n\u001b[0m                                                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#NOW USING AGENT ABOVE INSTEAD OF IMPORTANT AGENT:\n",
    "#USED TO BE: from nasim.agents.ql_agent import TabularQLearningAgent\n",
    "\n",
    "baseline_dqn_agent = DQNAgent(env, verbose=1, training_steps=200000, max_episodes=1000)\n",
    "baseline_dqn_agent.train()\n",
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")\n",
    "\n",
    "baseline_dqn_agent = DQNAgent(env, verbose=1, training_steps=, max_episodes=1000000) #10-20k steps, 10k episodes\n",
    "baseline_dqn_agent.train()\n",
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03a9e63d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m baseline_dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent(\u001b[43menv\u001b[49m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2000000\u001b[39m) \u001b[38;5;66;03m#10-20k steps, 10k episodes\u001b[39;00m\n\u001b[1;32m      2\u001b[0m baseline_dqn_agent\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m baseline_dqn_agent\u001b[38;5;241m.\u001b[39mrun_eval_episode(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "baseline_dqn_agent = DQNAgent(env, verbose=1, training_steps=2000000) #10-20k steps, 10k episodes\n",
    "baseline_dqn_agent.train()\n",
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "01a13184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN with config:\n",
      "{'batch_size': 32,\n",
      " 'env': <nasim.envs.environment.NASimEnv object at 0x7f7fe1337910>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_sizes': [64, 64],\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'replay_size': 10000,\n",
      " 'seed': None,\n",
      " 'self': <nasim.agents.dqn_agent.DQNAgent object at 0x7f7fe80167a0>,\n",
      " 'target_update_freq': 1000,\n",
      " 'training_steps': 150000,\n",
      " 'verbose': 1}\n",
      "\n",
      "Using Neural Network running on device=cuda:\n",
      "DQN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=207, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=64, out_features=72, bias=True)\n",
      ")\n",
      "\n",
      "Starting training\n",
      "Blocked host index:  -1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m baseline_dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150000\u001b[39m) \u001b[38;5;66;03m#10-20k steps, 10k episodes\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mbaseline_dqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m baseline_dqn_agent\u001b[38;5;241m.\u001b[39mrun_eval_episode(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 87\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_steps:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m og_env\n\u001b[0;32m---> 87\u001b[0m     ep_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_steps_remaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     ep_return, ep_steps, goal \u001b[38;5;241m=\u001b[39m ep_results\n\u001b[1;32m     89\u001b[0m     num_episodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[19], line 63\u001b[0m, in \u001b[0;36mrun_train_episode\u001b[0;34m(self, step_limit)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     62\u001b[0m next_o, r, done, env_step_limit_reached, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     65\u001b[0m loss, mean_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize()\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/nasim/agents/dqn_agent.py:60\u001b[0m, in \u001b[0;36mReplayMemory.store\u001b[0;34m(self, s, a, next_s, r, done)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstore\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a, next_s, r, done):\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ms_buf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptr] \u001b[38;5;241m=\u001b[39m s\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39ma_buf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptr] \u001b[38;5;241m=\u001b[39m a\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnext_s_buf[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mptr] \u001b[38;5;241m=\u001b[39m next_s\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "baseline_dqn_agent = DQNAgent(env, verbose=1, training_steps=150000) #10-20k steps, 10k episodes\n",
    "baseline_dqn_agent.train()\n",
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b41f798c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN with config:\n",
      "{'batch_size': 32,\n",
      " 'env': <nasim.envs.environment.NASimEnv object at 0x7f3692615e70>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_sizes': [64, 64],\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'replay_size': 10000,\n",
      " 'seed': None,\n",
      " 'self': <__main__.DQNAgent object at 0x7f34f4a52e90>,\n",
      " 'target_update_freq': 1000,\n",
      " 'training_steps': 150000,\n",
      " 'verbose': 1}\n",
      "\n",
      "Using Neural Network running on device=cuda:\n",
      "DQN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=207, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=64, out_features=72, bias=True)\n",
      ")\n",
      "\n",
      "Starting training\n",
      "\n",
      "Episode 10:\n",
      "\tsteps done = 4608 / 150000\n",
      "\treturn = -204.0\n",
      "\tgoal = True\n",
      "\n",
      "Episode 20:\n",
      "\tsteps done = 13583 / 150000\n",
      "\treturn = -1031.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 30:\n",
      "\tsteps done = 23583 / 150000\n",
      "\treturn = -1717.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 40:\n",
      "\tsteps done = 33243 / 150000\n",
      "\treturn = -953.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 50:\n",
      "\tsteps done = 43243 / 150000\n",
      "\treturn = -1105.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 60:\n",
      "\tsteps done = 53243 / 150000\n",
      "\treturn = -1084.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 70:\n",
      "\tsteps done = 63243 / 150000\n",
      "\treturn = -1089.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 80:\n",
      "\tsteps done = 73243 / 150000\n",
      "\treturn = -1061.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 90:\n",
      "\tsteps done = 83243 / 150000\n",
      "\treturn = -1052.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 100:\n",
      "\tsteps done = 93243 / 150000\n",
      "\treturn = -1017.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 110:\n",
      "\tsteps done = 103243 / 150000\n",
      "\treturn = -1021.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 120:\n",
      "\tsteps done = 113243 / 150000\n",
      "\treturn = -1021.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 130:\n",
      "\tsteps done = 123243 / 150000\n",
      "\treturn = -1050.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 140:\n",
      "\tsteps done = 133243 / 150000\n",
      "\treturn = -1013.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 150:\n",
      "\tsteps done = 143243 / 150000\n",
      "\treturn = -1014.0\n",
      "\tgoal = False\n",
      "Training complete\n",
      "\n",
      "Episode 157:\n",
      "\tsteps done = 150000 / 150000\n",
      "\treturn = -773.0\n",
      "\tgoal = False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(-1011.0, 1000, False)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c53f655d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1019.0, 1000, False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b7619ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN with config:\n",
      "{'batch_size': 32,\n",
      " 'env': <nasim.envs.environment.NASimEnv object at 0x7fa97ec9dcf0>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_sizes': [64, 64],\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'replay_size': 10000,\n",
      " 'seed': None,\n",
      " 'self': <nasim.agents.dqn_agent.DQNAgent object at 0x7fa8b47a7be0>,\n",
      " 'target_update_freq': 1000,\n",
      " 'training_steps': 50000000,\n",
      " 'verbose': 1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m env \u001b[38;5;241m=\u001b[39m nasim\u001b[38;5;241m.\u001b[39mmake_benchmark(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#Liam does it this way: env = nasim.load(\"small.yaml\")\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Initializing and training agent\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m dqn_agent\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/nasim/agents/dqn_agent.py:164\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, env, seed, lr, training_steps, batch_size, replay_size, final_epsilon, exploration_steps, gamma, hidden_sizes, target_update_freq, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Neural Network related attributes\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m                            \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m    161\u001b[0m                            \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m               \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 164\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUsing Neural Network running on device=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries, including which methods will be redefined\n",
    "import nasim\n",
    "import random\n",
    "from nasim.envs.action import Action\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "from nasim.envs.environment import NASimEnv\n",
    "\n",
    "# User-defined Python method to check whether the selected blocked_host is valid to select\n",
    "def check_host_valid(self, blocked_host):\n",
    "    if blocked_host == -1:\n",
    "        return\n",
    "    elif self.env.network.address_space[blocked_host] in self.env.network.get_sensitive_hosts():\n",
    "        raise SensitiveHostRemovalException\n",
    "    elif blocked_host == 0:\n",
    "        raise PublicHostRemovalException\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# Setting the method\n",
    "DQNAgent.check_host_valid = check_host_valid\n",
    "    \n",
    "# Redefining the DQNAgent run_train_episode method\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False #Unnecessary now with loop below using steps < step_limit\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        max_host_index = len(self.env.network.host_num_map) - 1\n",
    "        \n",
    "        # Choosing random host index to be invalid... try/catch loop until valid host selected to block. Note: If -1, no host will be marked invalid\n",
    "        blocked_host = -1\n",
    "        if self.steps_done > 0:\n",
    "            while True:\n",
    "                try:\n",
    "                    blocked_host = random.randint(-1,max_host_index)\n",
    "                    self.check_host_valid(blocked_host)\n",
    "                    break\n",
    "                except SensitiveHostRemovalException:\n",
    "                    pass\n",
    "                except PublicHostRemovalException:\n",
    "                    pass\n",
    "                \n",
    "        o, _ = self.env.reset()\n",
    "        \n",
    "        # If you wanted to see which host was blocked... used for the logging\n",
    "        print(\"Blocked host index:  \" + str(blocked_host))\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #steps < step_limit: #J: changed from env_step_limit_reached:     \n",
    "            #J: steps continuously updated at the bottom and will break as soon as step limit is reached\n",
    "            # Keep generating an action in the action space until it does not involve a blocked host\n",
    "            while True:\n",
    "                a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "                \n",
    "                if blocked_host == -1:\n",
    "                    break\n",
    "                else:\n",
    "                    action = self.env.action_space.get_action(a)\n",
    "                    target_host_index = self.env.network.host_num_map[action.target]\n",
    "                    if target_host_index != blocked_host:\n",
    "                        break\n",
    "                \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "# Setting the method\n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "# Training function... redefined because it wasn't converging originally\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    og_env = self.env\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        self.env = og_env\n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "# Set the method        \n",
    "DQNAgent.train = train\n",
    "\n",
    "# You can switch to a different benchmark if you want... like the scenario args posted or your own\n",
    "env = nasim.make_benchmark(\"small\")\n",
    "#Liam does it this way: env = nasim.load(\"small.yaml\")\n",
    "# Initializing and training agent\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=50000000)\n",
    "dqn_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab742de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.run_eval_episode(render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a1a496",
   "metadata": {},
   "source": [
    "## Past Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3f7037",
   "metadata": {},
   "source": [
    "This was some code that didn't end up working if you wanted to see a previous attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5eb05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "capacity = 10\n",
    "s_dims = (5,)\n",
    "s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "#test_tuple.resize(test_tuple, [3,2])\n",
    "\n",
    "print(s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3319db9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-e80566c61b5c>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-e80566c61b5c>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    prev_num_actions = self.num_actions\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nasim\n",
    "import random\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        \n",
    "        o = self.env.reset()\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #and steps < step_limit:\n",
    "            a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "        \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "    \n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    max_hosts = (self.env.scenario.get_description())['Hosts']\n",
    "    max_obs_dim = self.env.observation_space.shape\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        if self.steps_done > 0:\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.host_num_map)\n",
    "            print(self.env.network.subnets)\n",
    "            print(self.env.network.topology)\n",
    "            print(self.env.network.firewall)\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.address_space_bounds)\n",
    "            print(self.env.network.sensitive_addresses)\n",
    "            print(self.env.network.sensitive_hosts)\n",
    "\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = ReplayMemory(prev_replay_size,\n",
    "                                   #self.obs_dim,\n",
    "                                   #self.device)\n",
    "            \n",
    "            prev_observation_space = self.env.observation_space\n",
    "            prev_num_actions = self.num_actions\n",
    "            prev_obs_dim = self.obs_dim\n",
    "            prev_replay = self.replay\n",
    "            \n",
    "            scenario_args.update(num_hosts=random.randint(3,max_hosts))\n",
    "            \n",
    "            self.env =  nasim.generate(**scenario_args)\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = prev_replay\n",
    "            \n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "            \n",
    "DQNAgent.train = train\n",
    "\n",
    "print(scenario_args)\n",
    "env = nasim.generate(**scenario_args)\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=100000)\n",
    "dqn_agent.train()\n",
    "dqn_agent.run_eval_episode(render=args.render_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637e6dc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasim",
   "language": "python",
   "name": "nasim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
