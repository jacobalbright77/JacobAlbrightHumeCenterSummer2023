{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b013b4b7",
   "metadata": {},
   "source": [
    "## 7/19: File used for initial testing of running randomization agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a36f63",
   "metadata": {},
   "source": [
    "This is all some intro code just to visualize some of the settings and get a baseline. You can run through it if you want to get familiar with what the scenario arguments, agents, and training looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aca63cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = 'data.yaml'\n",
    "\n",
    "\n",
    "def writeToYAML():\n",
    "    with open(testfile, 'w') as f:\n",
    "        data = yaml.dump(curr_data, f, sort_keys=False, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6b36574",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_dict(dict_obj, indent = 0):\n",
    "    ''' Pretty Print nested dictionary with given indent level  \n",
    "    '''\n",
    "    # Iterate over all key-value pairs of dictionary\n",
    "    for key, value in dict_obj.items():\n",
    "        # If value is dict type, then print nested dict \n",
    "        if isinstance(value, dict):\n",
    "            print(' ' * indent, key, ':', '{')\n",
    "            print_nested_dict(value, indent + 4)\n",
    "            print(' ' * indent, '}')\n",
    "        else:\n",
    "            print(' ' * indent, key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "110824e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_args={\n",
    "    \"num_hosts\": 5,         # Number of hosts in the network \n",
    "    \n",
    "    \"num_services\": 3,      # Number of services on the network (ssh, ftp, http)\n",
    "    \n",
    "    \"num_os\": 2,            # Number of operatings systems on the network (windows, linux, etc)\n",
    "    \n",
    "    \"num_processes\": 2,     # Number of processes on the network (tomcat, daclsvc, etc)\n",
    "    \n",
    "    \"num_exploits\": None,   # Number of exploits to use\n",
    "    \n",
    "    \"num_privescs\": None,   # Number of privilege escalation actions\n",
    "    \n",
    "    \"r_sensitive\": 10,      # Reward for sensitive subnet documents (default 10)\n",
    "    \n",
    "    \"r_user\": 10,           # Reward for user subnet documents      (default 10)\n",
    "    \n",
    "    \"exploit_cost\": 1,      # Cost to use an exploit (default 1)\n",
    "    \n",
    "    \"exploit_probs\": 1.0,   # Sucess probability of exploits (default 1.0)\n",
    "    \n",
    "    \"privesc_cost\": 1,      # Cost of privilege escalation action (default 1)\n",
    "    \n",
    "    \"privesc_probs\": 1.0,   # Sucess probability of privilege escalation action (default 1.0)\n",
    "    \n",
    "    \"service_scan_cost\": 1, # Cost for a service scan (default 1)\n",
    "    \n",
    "    \"os_scan_cost\": 1,      # Cost for an OS scan (default 1)\n",
    "    \n",
    "    \"subnet_scan_cost\": 1,  # Cost for a subnet scan (default 1)\n",
    "    \n",
    "    \"process_scan_cost\": 1, # Cost for a process scan (default 1)\n",
    "    \n",
    "    \"uniform\": False,       # Whether to use uniform distribution or correlaed host configuration (default false)\n",
    "    \n",
    "    \"alpha_H\": 2.0,         # Scaling or concentration parameter for controlling corelation between host configurations (default 2.0)\n",
    "    \n",
    "    \"alpha_V\": 2.0,         # Scaling or concentration parameter for controlling corelation between services across host configruations (default 2.0)\n",
    "    \n",
    "    \"lambda_V\": 1.0,        # Parameter for controlling average number of services running per host configuration (default 1.0)\n",
    "    \n",
    "    \"restrictiveness\": 5,   # Maximum number of services allowed to pass through firewalls between zones (default 5)\n",
    "    \n",
    "    \"random_goal\": False,   # Whether to randomly assign the goal user host or not (default False)\n",
    "    \n",
    "    \"base_host_value\": 1,   # Value of non sensitive hosts (default 1)\n",
    "    \n",
    "    \"host_discovery_value\": 1,  # Value of discovering a host for the first time (default 1)\n",
    "    \n",
    "    \"seed\": None,           # Random number generator seed (default None)\n",
    "    \n",
    "    \"name\": None,           # Name of the scenario, one will be generated if None (default None)\n",
    "    \n",
    "    \"step_limit\": None}     # Max number of steps permitted in a single episode, None means no limit (default None)\n",
    "\n",
    "#Scenario Generator Parameter List: https://networkattacksimulator.readthedocs.io/en/latest/reference/scenarios/generator.html#scenario-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "271e45ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario Description: \n",
      "     Name : small\n",
      "     Type : static\n",
      "     Subnets : 5\n",
      "     Hosts : 8\n",
      "     OS : 2\n",
      "     Services : 3\n",
      "     Processes : 2\n",
      "     Exploits : 3\n",
      "     PrivEscs : 2\n",
      "     Actions : 72\n",
      "     Observation Dims : (9, 23)\n",
      "     States : 24576\n",
      "     Step Limit : 1000\n",
      "\n",
      "Scenario Dictionary: \n",
      "       subnets : [1, 1, 1, 5, 1]\n",
      "       topology : [[1, 1, 0, 0, 0], [1, 1, 1, 1, 0], [0, 1, 1, 1, 0], [0, 1, 1, 1, 1], [0, 0, 0, 1, 1]]\n",
      "       os : ['linux', 'windows']\n",
      "       services : ['ssh', 'ftp', 'http']\n",
      "       processes : ['tomcat', 'daclsvc']\n",
      "       sensitive_hosts : {\n",
      "           (2, 0) : 100\n",
      "           (4, 0) : 100\n",
      "       }\n",
      "       exploits : {\n",
      "           e_ssh : {\n",
      "               service : ssh\n",
      "               os : linux\n",
      "               prob : 0.9\n",
      "               cost : 3\n",
      "               access : 1\n",
      "           }\n",
      "           e_ftp : {\n",
      "               service : ftp\n",
      "               os : windows\n",
      "               prob : 0.6\n",
      "               cost : 1\n",
      "               access : 1\n",
      "           }\n",
      "           e_http : {\n",
      "               service : http\n",
      "               os : None\n",
      "               prob : 0.9\n",
      "               cost : 2\n",
      "               access : 1\n",
      "           }\n",
      "       }\n",
      "       privilege_escalation : {\n",
      "           pe_tomcat : {\n",
      "               process : tomcat\n",
      "               os : linux\n",
      "               prob : 1.0\n",
      "               cost : 1\n",
      "               access : 2\n",
      "           }\n",
      "           pe_daclsvc : {\n",
      "               process : daclsvc\n",
      "               os : windows\n",
      "               prob : 1.0\n",
      "               cost : 1\n",
      "               access : 2\n",
      "           }\n",
      "       }\n",
      "       os_scan_cost : 1\n",
      "       service_scan_cost : 1\n",
      "       subnet_scan_cost : 1\n",
      "       process_scan_cost : 1\n",
      "       firewall : {\n",
      "           (0, 1) : ['http']\n",
      "           (1, 0) : []\n",
      "           (1, 2) : ['ssh']\n",
      "           (2, 1) : ['ssh']\n",
      "           (1, 3) : []\n",
      "           (3, 1) : ['ssh']\n",
      "           (2, 3) : ['http']\n",
      "           (3, 2) : ['ftp']\n",
      "           (3, 4) : ['ssh', 'ftp']\n",
      "           (4, 3) : ['ftp']\n",
      "       }\n",
      "       host : {\n",
      "           (1, 0) : Host: {\n",
      "\taddress: (1, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t\twindows: False\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: False\n",
      "\t\thttp: True\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (2, 0) : Host: {\n",
      "\taddress: (2, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 100.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t\twindows: False\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: True\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: True\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 0) : Host: {\n",
      "\taddress: (3, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 1) : Host: {\n",
      "\taddress: (3, 1)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: True\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 2) : Host: {\n",
      "\taddress: (3, 2)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 3) : Host: {\n",
      "\taddress: (3, 3)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (3, 4) : Host: {\n",
      "\taddress: (3, 4)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: False\n",
      "\t\twindows: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: False\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: False\n",
      "\t\tdaclsvc: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "           (4, 0) : Host: {\n",
      "\taddress: (4, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 100.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t\twindows: False\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: True\n",
      "\t\tftp: True\n",
      "\t\thttp: False\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: True\n",
      "\t\tdaclsvc: False\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "       }\n",
      "       step_limit : 1000\n"
     ]
    }
   ],
   "source": [
    "import nasim\n",
    "import json\n",
    "env = nasim.generate(**scenario_args)\n",
    "env = nasim.make_benchmark(\"huge-gen\")\n",
    "env = nasim.load(\"unreachable.yaml\")\n",
    "env2 = env = nasim.make_benchmark(\"small\")\n",
    "\n",
    "\n",
    "scenario_desc = env.scenario.get_description() #get_description found in scenario.py file under nasim->scenarios\n",
    "scenario_dict = env.scenario.scenario_dict\n",
    "#scenario_exploit_map = env.scenario.exploit_map # A nested dictionary for all exploits in scenario.\n",
    "#scenario_privesc_map = env.scenario.privesc_map # A nested dictionary for all privilege escalation actions in scenario.\n",
    "\n",
    "print(\"Scenario Description: \")\n",
    "print_nested_dict(scenario_desc,4)\n",
    "\n",
    "print(\"\\nScenario Dictionary: \")\n",
    "print_nested_dict(scenario_dict,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cdfa596",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.get_minimum_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ab5317a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--render_eval] [--lr LR]\n",
      "                             [-t TRAINING_STEPS] [--batch_size BATCH_SIZE]\n",
      "                             [--seed SEED] [--replay_size REPLAY_SIZE]\n",
      "                             [--final_epsilon FINAL_EPSILON]\n",
      "                             [--init_epsilon INIT_EPSILON]\n",
      "                             [-e EXPLORATION_STEPS] [--gamma GAMMA] [--quite]\n",
      "                             env_name\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    " \"\"\"An example Tabular, epsilon greedy Q-Learning Agent.\n",
    "\n",
    "This agent does not use an Experience replay (see the 'ql_replay_agent.py')\n",
    "\n",
    "It uses pytorch 1.5+ tensorboard library for logging (HINT: these dependencies\n",
    "can be installed by running pip install nasim[dqn])\n",
    "\n",
    "To run 'tiny' benchmark scenario with default settings, run the following from\n",
    "the nasim/agents dir:\n",
    "\n",
    "$ python ql_agent.py tiny\n",
    "\n",
    "To see detailed results using tensorboard:\n",
    "\n",
    "$ tensorboard --logdir runs/\n",
    "\n",
    "To see available hyperparameters:\n",
    "\n",
    "$ python ql_agent.py --help\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "This is by no means a state of the art implementation of Tabular Q-Learning.\n",
    "It is designed to be an example implementation that can be used as a reference\n",
    "for building your own agents and for simple experimental comparisons.\n",
    "\"\"\"\n",
    "import random\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import nasim\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError as e:\n",
    "    from gymnasium import error\n",
    "    raise error.DependencyNotInstalled(\n",
    "        f\"{e}. (HINT: you can install tabular_q_learning_agent dependencies \"\n",
    "        \"by running 'pip install nasim[dqn]'.)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class TabularQFunction:\n",
    "    \"\"\"Tabular Q-Function \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions):\n",
    "        self.q_func = dict()\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = str(x.astype(int))\n",
    "        if x not in self.q_func:\n",
    "            self.q_func[x] = np.zeros(self.num_actions, dtype=np.float32)\n",
    "        return self.q_func[x]\n",
    "\n",
    "    def forward_batch(self, x_batch):\n",
    "        return np.asarray([self.forward(x) for x in x_batch])\n",
    "\n",
    "    def update_batch(self, s_batch, a_batch, delta_batch):\n",
    "        for s, a, delta in zip(s_batch, a_batch, delta_batch):\n",
    "            q_vals = self.forward(s)\n",
    "            q_vals[a] += delta\n",
    "\n",
    "    def update(self, s, a, delta):\n",
    "        q_vals = self.forward(s)\n",
    "        q_vals[a] += delta\n",
    "\n",
    "    def get_action(self, x):\n",
    "        return int(self.forward(x).argmax())\n",
    "\n",
    "    def display(self):\n",
    "        pprint(self.q_func)\n",
    "\n",
    "\n",
    "class TabularQLearningAgent:\n",
    "    \"\"\"A Tabular. epsilon greedy Q-Learning Agent using Experience Replay \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 seed=None,\n",
    "                 lr=0.001,\n",
    "                 training_steps=10000,\n",
    "                 final_epsilon=0.05,\n",
    "                 exploration_steps=10000,\n",
    "                 gamma=0.99,\n",
    "                 verbose=True,\n",
    "                 **kwargs):\n",
    "\n",
    "        # This implementation only works for flat actions\n",
    "        assert env.flat_actions\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(\"\\nRunning Tabular Q-Learning with config:\")\n",
    "            pprint(locals())\n",
    "\n",
    "        # set seeds\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # envirnment setup\n",
    "        self.env = env\n",
    "\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "\n",
    "        # logger setup\n",
    "        self.logger = SummaryWriter()\n",
    "\n",
    "        # Training related attributes\n",
    "        self.lr = lr\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.epsilon_schedule = np.linspace(\n",
    "            1.0, self.final_epsilon, self.exploration_steps\n",
    "        )\n",
    "        self.discount = gamma\n",
    "        self.training_steps = training_steps\n",
    "        self.steps_done = 0\n",
    "\n",
    "        # Q-Function\n",
    "        self.qfunc = TabularQFunction(self.num_actions)\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        if self.steps_done < self.exploration_steps:\n",
    "            return self.epsilon_schedule[self.steps_done]\n",
    "        return self.final_epsilon\n",
    "\n",
    "    def get_egreedy_action(self, o, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            return self.qfunc.get_action(o)\n",
    "        return random.randint(0, self.num_actions-1)\n",
    "\n",
    "    def optimize(self, s, a, next_s, r, done):\n",
    "        # get q_val for state and action performed in that state\n",
    "        q_vals_raw = self.qfunc.forward(s)\n",
    "        q_val = q_vals_raw[a]\n",
    "\n",
    "        # get target q val = max val of next state\n",
    "        target_q_val = self.qfunc.forward(next_s).max()\n",
    "        target = r + self.discount * (1-done) * target_q_val\n",
    "\n",
    "        # calculate error and update\n",
    "        td_error = target - q_val\n",
    "        td_delta = self.lr * td_error\n",
    "\n",
    "        # optimize the model\n",
    "        self.qfunc.update(s, a, td_delta)\n",
    "\n",
    "        s_value = q_vals_raw.max()\n",
    "        return td_error, s_value\n",
    "\n",
    "    def train(self):\n",
    "        if self.verbose:\n",
    "            print(\"\\nStarting training\")\n",
    "\n",
    "        num_episodes = 0\n",
    "        training_steps_remaining = self.training_steps\n",
    "\n",
    "        while self.steps_done < self.training_steps:\n",
    "            ep_results = self.run_train_episode(training_steps_remaining)\n",
    "            ep_return, ep_steps, goal = ep_results\n",
    "            num_episodes += 1\n",
    "            training_steps_remaining -= ep_steps\n",
    "\n",
    "            self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "            self.logger.add_scalar(\n",
    "                \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_return\", ep_return, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_steps\", ep_steps, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_goal_reached\", int(goal), self.steps_done\n",
    "            )\n",
    "\n",
    "            if num_episodes % 10 == 0 and self.verbose:\n",
    "                print(f\"\\nEpisode {num_episodes}:\")\n",
    "                print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                      f\"{self.training_steps}\")\n",
    "                print(f\"\\treturn = {ep_return}\")\n",
    "                print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "        self.logger.close()\n",
    "        if self.verbose:\n",
    "            print(\"Training complete\")\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    def run_train_episode(self, step_limit):\n",
    "        s, _ = self.env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        while not done and not env_step_limit_reached and steps < step_limit:\n",
    "            a = self.get_egreedy_action(s, self.get_epsilon())\n",
    "\n",
    "            next_s, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.steps_done += 1\n",
    "            td_error, s_value = self.optimize(s, a, next_s, r, done)\n",
    "            self.logger.add_scalar(\"td_error\", td_error, self.steps_done)\n",
    "            self.logger.add_scalar(\"s_value\", s_value, self.steps_done)\n",
    "\n",
    "            s = next_s\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "    def run_eval_episode(self,\n",
    "                         env=None,\n",
    "                         render=False,\n",
    "                         eval_epsilon=0.05,\n",
    "                         render_mode=\"human\"):\n",
    "        if env is None:\n",
    "            env = self.env\n",
    "\n",
    "        original_render_mode = env.render_mode\n",
    "        env.render_mode = render_mode\n",
    "\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        line_break = \"=\"*60\n",
    "        if render:\n",
    "            print(\"\\n\" + line_break)\n",
    "            print(f\"Running EVALUATION using epsilon = {eval_epsilon:.4f}\")\n",
    "            print(line_break)\n",
    "            env.render()\n",
    "            input(\"Initial state. Press enter to continue..\")\n",
    "\n",
    "        while not done and not env_step_limit_reached:\n",
    "            a = self.get_egreedy_action(s, eval_epsilon)\n",
    "            next_s, r, done, env_step_limit_reached, _ = env.step(a)\n",
    "            s = next_s\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "            if render:\n",
    "                print(\"\\n\" + line_break)\n",
    "                print(f\"Step {steps}\")\n",
    "                print(line_break)\n",
    "                print(f\"Action Performed = {env.action_space.get_action(a)}\")\n",
    "                env.render()\n",
    "                print(f\"Reward = {r}\")\n",
    "                print(f\"Done = {done}\")\n",
    "                print(f\"Step limit reached = {env_step_limit_reached}\")\n",
    "                input(\"Press enter to continue..\")\n",
    "\n",
    "                if done or env_step_limit_reached:\n",
    "                    print(\"\\n\" + line_break)\n",
    "                    print(\"EPISODE FINISHED\")\n",
    "                    print(line_break)\n",
    "                    print(f\"Goal reached = {env.goal_reached()}\")\n",
    "                    print(f\"Total steps = {steps}\")\n",
    "                    print(f\"Total reward = {episode_return}\")\n",
    "\n",
    "        env.render_mode = original_render_mode\n",
    "        return episode_return, steps, env.goal_reached()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"env_name\", type=str, help=\"benchmark scenario name\")\n",
    "    parser.add_argument(\"--render_eval\", action=\"store_true\",\n",
    "                        help=\"Renders final policy\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001,\n",
    "                        help=\"Learning rate (default=0.001)\")\n",
    "    parser.add_argument(\"-t\", \"--training_steps\", type=int, default=10000,\n",
    "                        help=\"training steps (default=10000)\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                        help=\"(default=32)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0,\n",
    "                        help=\"(default=0)\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=100000,\n",
    "                        help=\"(default=100000)\")\n",
    "    parser.add_argument(\"--final_epsilon\", type=float, default=0.05,\n",
    "                        help=\"(default=0.05)\")\n",
    "    parser.add_argument(\"--init_epsilon\", type=float, default=1.0,\n",
    "                        help=\"(default=1.0)\")\n",
    "    parser.add_argument(\"-e\", \"--exploration_steps\", type=int, default=10000,\n",
    "                        help=\"(default=10000)\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "                        help=\"(default=0.99)\")\n",
    "    parser.add_argument(\"--quite\", action=\"store_false\",\n",
    "                        help=\"Run in Quite mode\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    env = nasim.make_benchmark(\n",
    "        args.env_name,\n",
    "        args.seed,\n",
    "        fully_obs=True,\n",
    "        flat_actions=True,\n",
    "        flat_obs=True\n",
    "    )\n",
    "    ql_agent = TabularQLearningAgent(\n",
    "        env, verbose=args.quite, **vars(args)\n",
    "    )\n",
    "    #ql_agent.train()\n",
    "    #ql_agent.run_eval_episode(render=args.render_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b52ba629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Tabular Q-Learning with config:\n",
      "{'env': <nasim.envs.environment.NASimEnv object at 0x7f26cea3a200>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'seed': None,\n",
      " 'self': <__main__.TabularQLearningAgent object at 0x7f28302f5b40>,\n",
      " 'training_steps': 500000,\n",
      " 'verbose': 1}\n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#NOW USING AGENT ABOVE INSTEAD OF IMPORTING AGENT:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#USED TO BE: from nasim.agents.ql_agent import TabularQLearningAgent\u001b[39;00m\n\u001b[1;32m      4\u001b[0m ql_agent \u001b[38;5;241m=\u001b[39m TabularQLearningAgent(env2, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500000\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m training_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mql_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 166\u001b[0m, in \u001b[0;36mTabularQLearningAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    163\u001b[0m training_steps_remaining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_steps\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_steps:\n\u001b[0;32m--> 166\u001b[0m     ep_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_steps_remaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     ep_return, ep_steps, goal \u001b[38;5;241m=\u001b[39m ep_results\n\u001b[1;32m    168\u001b[0m     num_episodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[14], line 213\u001b[0m, in \u001b[0;36mTabularQLearningAgent.run_train_episode\u001b[0;34m(self, step_limit)\u001b[0m\n\u001b[1;32m    211\u001b[0m next_s, r, done, env_step_limit_reached, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 213\u001b[0m td_error, s_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtd_error\u001b[39m\u001b[38;5;124m\"\u001b[39m, td_error, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s_value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done)\n",
      "Cell \u001b[0;32mIn[14], line 141\u001b[0m, in \u001b[0;36mTabularQLearningAgent.optimize\u001b[0;34m(self, s, a, next_s, r, done)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, a, next_s, r, done):\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# get q_val for state and action performed in that state\u001b[39;00m\n\u001b[0;32m--> 141\u001b[0m     q_vals_raw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     q_val \u001b[38;5;241m=\u001b[39m q_vals_raw[a]\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# get target q val = max val of next state\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 56\u001b[0m, in \u001b[0;36mTabularQFunction.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 56\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_func:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_func[x] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/numpy/core/arrayprint.py:1612\u001b[0m, in \u001b[0;36m_array_str_implementation\u001b[0;34m(a, max_line_width, precision, suppress_small, array2string)\u001b[0m\n\u001b[1;32m   1606\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ():\n\u001b[1;32m   1607\u001b[0m     \u001b[38;5;66;03m# obtain a scalar and call str on it, avoiding problems for subclasses\u001b[39;00m\n\u001b[1;32m   1608\u001b[0m     \u001b[38;5;66;03m# for which indexing with () returns a 0d instead of a scalar by using\u001b[39;00m\n\u001b[1;32m   1609\u001b[0m     \u001b[38;5;66;03m# ndarray's getindex. Also guard against recursive 0d object arrays.\u001b[39;00m\n\u001b[1;32m   1610\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _guarded_repr_or_str(np\u001b[38;5;241m.\u001b[39mndarray\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(a, ()))\n\u001b[0;32m-> 1612\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray2string\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_line_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/numpy/core/arrayprint.py:711\u001b[0m, in \u001b[0;36marray2string\u001b[0;34m(a, max_line_width, precision, suppress_small, separator, prefix, style, formatter, threshold, edgeitems, sign, floatmode, suffix, legacy)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_array2string_dispatcher, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21marray2string\u001b[39m(a, max_line_width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    563\u001b[0m                  suppress_small\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m, prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    564\u001b[0m                  style\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39m_NoValue, formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    565\u001b[0m                  edgeitems\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sign\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, floatmode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, suffix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    566\u001b[0m                  \u001b[38;5;241m*\u001b[39m, legacy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;124;03m    Return a string representation of an array.\u001b[39;00m\n\u001b[1;32m    569\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    708\u001b[0m \n\u001b[1;32m    709\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 711\u001b[0m     overrides \u001b[38;5;241m=\u001b[39m \u001b[43m_make_options_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprecision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgeitems\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mmax_line_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuppress_small\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43msign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfloatmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlegacy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    714\u001b[0m     options \u001b[38;5;241m=\u001b[39m _format_options\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    715\u001b[0m     options\u001b[38;5;241m.\u001b[39mupdate(overrides)\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/numpy/core/arrayprint.py:64\u001b[0m, in \u001b[0;36m_make_options_dict\u001b[0;34m(precision, threshold, edgeitems, linewidth, suppress, nanstr, infstr, sign, formatter, floatmode, legacy)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcontextlib\u001b[39;00m\n\u001b[1;32m     49\u001b[0m _format_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medgeitems\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m,  \u001b[38;5;66;03m# repr N leading and trailing items of each dimension\u001b[39;00m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthreshold\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1000\u001b[39m,  \u001b[38;5;66;03m# total items > triggers array summarization\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# str/False on the way in/out.\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlegacy\u001b[39m\u001b[38;5;124m'\u001b[39m: sys\u001b[38;5;241m.\u001b[39mmaxsize}\n\u001b[0;32m---> 64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_options_dict\u001b[39m(precision\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, edgeitems\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     65\u001b[0m                        linewidth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, suppress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, nanstr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, infstr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     66\u001b[0m                        sign\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, floatmode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, legacy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03m    Make a dictionary out of the non-None arguments, plus conversion of\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m    *legacy* and sanity checks.\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     options \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m()\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m}\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#NOW USING AGENT ABOVE INSTEAD OF IMPORTING AGENT:\n",
    "#USED TO BE: from nasim.agents.ql_agent import TabularQLearningAgent\n",
    "\n",
    "ql_agent = TabularQLearningAgent(env2, verbose=1, training_steps=500000)\n",
    "training_outputs = ql_agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dea44b",
   "metadata": {},
   "source": [
    "## Current Code \n",
    "Here is the main code to test/run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39f6f9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial scenario arguments... we will be editing the number of hosts by marking actions involving them as invalid\n",
    "scenario_args={\n",
    "    \"num_hosts\": 5,         # Number of hosts in the network \n",
    "    \n",
    "    \"num_services\": 3,      # Number of services on the network (ssh, ftp, http)\n",
    "    \n",
    "    \"num_os\": 2,            # Number of operatings systems on the network (windows, linux, etc)\n",
    "    \n",
    "    \"num_processes\": 2,     # Number of processes on the network (tomcat, daclsvc, etc)\n",
    "    \n",
    "    \"num_exploits\": None,   # Number of exploits to use\n",
    "    \n",
    "    \"num_privescs\": None,   # Number of privilege escalation actions\n",
    "    \n",
    "    \"r_sensitive\": 10,      # Reward for sensitive subnet documents (default 10)\n",
    "    \n",
    "    \"r_user\": 10,           # Reward for user subnet documents      (default 10)\n",
    "    \n",
    "    \"exploit_cost\": 1,      # Cost to use an exploit (default 1)\n",
    "    \n",
    "    \"exploit_probs\": 1.0,   # Sucess probability of exploits (default 1.0)\n",
    "    \n",
    "    \"privesc_cost\": 1,      # Cost of privilege escalation action (default 1)\n",
    "    \n",
    "    \"privesc_probs\": 1.0,   # Sucess probability of privilege escalation action (default 1.0)\n",
    "    \n",
    "    \"service_scan_cost\": 1, # Cost for a service scan (default 1)\n",
    "    \n",
    "    \"os_scan_cost\": 1,      # Cost for an OS scan (default 1)\n",
    "    \n",
    "    \"subnet_scan_cost\": 1,  # Cost for a subnet scan (default 1)\n",
    "    \n",
    "    \"process_scan_cost\": 1, # Cost for a process scan (default 1)\n",
    "    \n",
    "    \"uniform\": False,       # Whether to use uniform distribution or correlaed host configuration (default false)\n",
    "    \n",
    "    \"alpha_H\": 2.0,         # Scaling or concentration parameter for controlling corelation between host configurations (default 2.0)\n",
    "    \n",
    "    \"alpha_V\": 2.0,         # Scaling or concentration parameter for controlling corelation between services across host configruations (default 2.0)\n",
    "    \n",
    "    \"lambda_V\": 1.0,        # Parameter for controlling average number of services running per host configuration (default 1.0)\n",
    "    \n",
    "    \"restrictiveness\": 5,   # Maximum number of services allowed to pass through firewalls between zones (default 5)\n",
    "    \n",
    "    \"random_goal\": False,   # Whether to randomly assign the goal user host or not (default False)\n",
    "    \n",
    "    \"base_host_value\": 1,   # Value of non sensitive hosts (default 1)\n",
    "    \n",
    "    \"host_discovery_value\": 1,  # Value of discovering a host for the first time (default 1)\n",
    "    \n",
    "    \"seed\": None,           # Random number generator seed (default None)\n",
    "    \n",
    "    \"name\": None,           # Name of the scenario, one will be generated if None (default None)\n",
    "    \n",
    "    \"step_limit\": None}     # Max number of steps permitted in a single episode, None means no limit (default None)\n",
    "\n",
    "#Scenario Generator Parameter List: https://networkattacksimulator.readthedocs.io/en/latest/reference/scenarios/generator.html#scenario-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e50f6445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Python user-defined exceptions\n",
    "class SensitiveHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (sensitive host needs to remain in network)\"\n",
    "    pass\n",
    "\n",
    "class PublicHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (public host to enter the network... specific to this configuration)\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de5ac931",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--render_eval] [-o]\n",
      "                             [--hidden_sizes [HIDDEN_SIZES ...]] [--lr LR]\n",
      "                             [-t TRAINING_STEPS] [--batch_size BATCH_SIZE]\n",
      "                             [--target_update_freq TARGET_UPDATE_FREQ]\n",
      "                             [--seed SEED] [--replay_size REPLAY_SIZE]\n",
      "                             [--final_epsilon FINAL_EPSILON]\n",
      "                             [--init_epsilon INIT_EPSILON]\n",
      "                             [--exploration_steps EXPLORATION_STEPS]\n",
      "                             [--gamma GAMMA] [--quite]\n",
      "                             env_name\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/envs/nasim/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3450: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# DEFAULT DQN AGENT AS CONTROL\n",
    "\n",
    "\"\"\"An example DQN Agent.\n",
    "\n",
    "It uses pytorch 1.5+ and tensorboard libraries (HINT: these dependencies can\n",
    "be installed by running pip install nasim[dqn])\n",
    "\n",
    "To run 'tiny' benchmark scenario with default settings, run the following from\n",
    "the nasim/agents dir:\n",
    "\n",
    "$ python dqn_agent.py tiny\n",
    "\n",
    "To see detailed results using tensorboard:\n",
    "\n",
    "$ tensorboard --logdir runs/\n",
    "\n",
    "To see available hyperparameters:\n",
    "\n",
    "$ python dqn_agent.py --help\n",
    "\n",
    "Notes\n",
    "-----\n",
    "\n",
    "This is by no means a state of the art implementation of DQN, but is designed\n",
    "to be an example implementation that can be used as a reference for building\n",
    "your own agents.\n",
    "\"\"\"\n",
    "import random\n",
    "from pprint import pprint\n",
    "\n",
    "from gymnasium import error\n",
    "import numpy as np\n",
    "\n",
    "import nasim\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.optim as optim\n",
    "    import torch.nn.functional as F\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError as e:\n",
    "    raise error.DependencyNotInstalled(\n",
    "        f\"{e}. (HINT: you can install dqn_agent dependencies by running \"\n",
    "        \"'pip install nasim[dqn]'.)\"\n",
    "    )\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "\n",
    "    def __init__(self, capacity, s_dims, device=\"cpu\"):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "        self.a_buf = np.zeros((capacity, 1), dtype=np.int64)\n",
    "        self.next_s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "        self.r_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.done_buf = np.zeros(capacity, dtype=np.float32)\n",
    "        self.ptr, self.size = 0, 0\n",
    "\n",
    "    def store(self, s, a, next_s, r, done):\n",
    "        self.s_buf[self.ptr] = s\n",
    "        self.a_buf[self.ptr] = a\n",
    "        self.next_s_buf[self.ptr] = next_s\n",
    "        self.r_buf[self.ptr] = r\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.capacity\n",
    "        self.size = min(self.size+1, self.capacity)\n",
    "\n",
    "    def sample_batch(self, batch_size):\n",
    "        sample_idxs = np.random.choice(self.size, batch_size)\n",
    "        batch = [self.s_buf[sample_idxs],\n",
    "                 self.a_buf[sample_idxs],\n",
    "                 self.next_s_buf[sample_idxs],\n",
    "                 self.r_buf[sample_idxs],\n",
    "                 self.done_buf[sample_idxs]]\n",
    "        return [torch.from_numpy(buf).to(self.device) for buf in batch]\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"A simple Deep Q-Network \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim, layers, num_actions):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_dim[0], layers[0])])\n",
    "        for l in range(1, len(layers)):\n",
    "            self.layers.append(nn.Linear(layers[l-1], layers[l]))\n",
    "        self.out = nn.Linear(layers[-1], num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = F.relu(layer(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def save_DQN(self, file_path):\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "    def load_DQN(self, file_path):\n",
    "        self.load_state_dict(torch.load(file_path))\n",
    "\n",
    "    def get_action(self, x):\n",
    "        with torch.no_grad():\n",
    "            if len(x.shape) == 1:\n",
    "                x = x.view(1, -1)\n",
    "            return self.forward(x).max(1)[1]\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"A simple Deep Q-Network Agent \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 seed=None,\n",
    "                 lr=0.001,\n",
    "                 training_steps=20000,\n",
    "                 batch_size=32,\n",
    "                 replay_size=10000,\n",
    "                 final_epsilon=0.05,\n",
    "                 exploration_steps=10000,\n",
    "                 gamma=0.99,\n",
    "                 hidden_sizes=[64, 64],\n",
    "                 target_update_freq=1000,\n",
    "                 verbose=True,\n",
    "                 **kwargs):\n",
    "\n",
    "        # This DQN implementation only works for flat actions\n",
    "        assert env.flat_actions\n",
    "        self.verbose = verbose\n",
    "        if self.verbose:\n",
    "            print(f\"\\nRunning DQN with config:\")\n",
    "            pprint(locals())\n",
    "\n",
    "        # set seeds\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            np.random.seed(self.seed)\n",
    "\n",
    "        # environment setup\n",
    "        self.env = env\n",
    "\n",
    "        self.num_actions = self.env.action_space.n\n",
    "        self.obs_dim = self.env.observation_space.shape\n",
    "\n",
    "        # logger setup\n",
    "        self.logger = SummaryWriter()\n",
    "\n",
    "        # Training related attributes\n",
    "        self.lr = lr\n",
    "        self.exploration_steps = exploration_steps\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.epsilon_schedule = np.linspace(1.0,\n",
    "                                            self.final_epsilon,\n",
    "                                            self.exploration_steps)\n",
    "        self.batch_size = batch_size\n",
    "        self.discount = gamma\n",
    "        self.training_steps = training_steps\n",
    "        self.steps_done = 0\n",
    "\n",
    "        # Neural Network related attributes\n",
    "        self.device = torch.device(\"cuda\"\n",
    "                                   if torch.cuda.is_available()\n",
    "                                   else \"cpu\")\n",
    "        self.dqn = DQN(self.obs_dim,\n",
    "                       hidden_sizes,\n",
    "                       self.num_actions).to(self.device)\n",
    "        if self.verbose:\n",
    "            print(f\"\\nUsing Neural Network running on device={self.device}:\")\n",
    "            print(self.dqn)\n",
    "\n",
    "        self.target_dqn = DQN(self.obs_dim,\n",
    "                              hidden_sizes,\n",
    "                              self.num_actions).to(self.device)\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr=self.lr)\n",
    "        self.loss_fn = nn.SmoothL1Loss()\n",
    "\n",
    "        # replay setup\n",
    "        self.replay = ReplayMemory(replay_size,\n",
    "                                   self.obs_dim,\n",
    "                                   self.device)\n",
    "\n",
    "    def save(self, save_path):\n",
    "        self.dqn.save_DQN(save_path)\n",
    "\n",
    "    def load(self, load_path):\n",
    "        self.dqn.load_DQN(load_path)\n",
    "\n",
    "    def get_epsilon(self):\n",
    "        if self.steps_done < self.exploration_steps:\n",
    "            return self.epsilon_schedule[self.steps_done]\n",
    "        return self.final_epsilon\n",
    "\n",
    "    def get_egreedy_action(self, o, epsilon):\n",
    "        if random.random() > epsilon:\n",
    "            o = torch.from_numpy(o).float().to(self.device)\n",
    "            return self.dqn.get_action(o).cpu().item()\n",
    "        return random.randint(0, self.num_actions-1)\n",
    "\n",
    "    def optimize(self):\n",
    "        batch = self.replay.sample_batch(self.batch_size)\n",
    "        s_batch, a_batch, next_s_batch, r_batch, d_batch = batch\n",
    "\n",
    "        # get q_vals for each state and the action performed in that state\n",
    "        q_vals_raw = self.dqn(s_batch)\n",
    "        q_vals = q_vals_raw.gather(1, a_batch).squeeze()\n",
    "\n",
    "        # get target q val = max val of next state\n",
    "        with torch.no_grad():\n",
    "            target_q_val_raw = self.target_dqn(next_s_batch)\n",
    "            target_q_val = target_q_val_raw.max(1)[0]\n",
    "            target = r_batch + self.discount*(1-d_batch)*target_q_val\n",
    "\n",
    "        # calculate loss\n",
    "        loss = self.loss_fn(q_vals, target)\n",
    "\n",
    "        # optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.steps_done % self.target_update_freq == 0:\n",
    "            self.target_dqn.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "        q_vals_max = q_vals_raw.max(1)[0]\n",
    "        mean_v = q_vals_max.mean().item()\n",
    "        return loss.item(), mean_v\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        import numpy as np\n",
    "        import matplotlib\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        \n",
    "        def plot_average_bar_chart(avg_list):\n",
    "            x_labels = range(1, len(avg_list) + 1) #list of labels for x-axis\n",
    "            plt.plot(x_labels, avg_list) #Creating bar chart\n",
    "            plt.xlabel('Step Number')\n",
    "            plt.ylabel('Reward')\n",
    "            plt.title('Average Reward Over Time')\n",
    "            plt.show()\n",
    "\n",
    "        if self.verbose:\n",
    "            print(\"\\nStarting training\")\n",
    "\n",
    "        num_episodes = 0\n",
    "        training_steps_remaining = self.training_steps\n",
    "        \n",
    "        elems_to_avg = [] #Jacob edit\n",
    "        avg_plot_list = []\n",
    "\n",
    "        while self.steps_done < self.training_steps:\n",
    "            ep_results = self.run_train_episode(training_steps_remaining)\n",
    "            ep_return, ep_steps, goal = ep_results #ep_return, ep_steps, and goal equal ep_results\n",
    "            num_episodes += 1\n",
    "            training_steps_remaining -= ep_steps\n",
    "\n",
    "            self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "            self.logger.add_scalar(\n",
    "                \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_return\", ep_return, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_steps\", ep_steps, self.steps_done\n",
    "            )\n",
    "            self.logger.add_scalar(\n",
    "                \"episode_goal_reached\", int(goal), self.steps_done\n",
    "            )\n",
    "\n",
    "            if num_episodes % 50 == 0 and self.verbose:\n",
    "                avg = (sum(elems_to_avg) / len(elems_to_avg))\n",
    "                print(f\"\\taverage = {avg}\")\n",
    "            \n",
    "            if num_episodes % 10 == 0 and self.verbose:\n",
    "                print(f\"\\nEpisode {num_episodes}:\")\n",
    "                print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                      f\"{self.training_steps}\")\n",
    "                print(f\"\\treturn = {ep_return}\")\n",
    "                print(f\"\\tgoal = {goal}\")\n",
    "                \n",
    "                elems_to_avg.append(ep_return) #Jacob edit\n",
    "            \n",
    "                \n",
    "\n",
    "        self.logger.close()\n",
    "        if self.verbose:\n",
    "            print(\"Training complete\")\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "            \n",
    "            print(elems_to_avg)\n",
    "            \n",
    "            plot_average_bar_chart(elems_to_avg)\n",
    "            \n",
    "            \n",
    "\n",
    "    def run_train_episode(self, step_limit):\n",
    "        o, _ = self.env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        while not done and not env_step_limit_reached and steps < step_limit:\n",
    "            a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "\n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            self.logger.add_scalar(\"loss\", loss, self.steps_done)\n",
    "            self.logger.add_scalar(\"mean_v\", mean_v, self.steps_done)\n",
    "\n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "    def run_eval_episode(self,\n",
    "                         env=None,\n",
    "                         render=False,\n",
    "                         eval_epsilon=0.05,\n",
    "                         render_mode=\"human\"):\n",
    "        if env is None:\n",
    "            env = self.env\n",
    "\n",
    "        original_render_mode = env.render_mode\n",
    "        env.render_mode = render_mode\n",
    "\n",
    "        o, _ = env.reset()\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "\n",
    "        line_break = \"=\"*60\n",
    "        if render:\n",
    "            print(\"\\n\" + line_break)\n",
    "            print(f\"Running EVALUATION using epsilon = {eval_epsilon:.4f}\")\n",
    "            print(line_break)\n",
    "            env.render()\n",
    "            input(\"Initial state. Press enter to continue..\")\n",
    "\n",
    "        while not done and not env_step_limit_reached:\n",
    "            a = self.get_egreedy_action(o, eval_epsilon)\n",
    "            next_o, r, done, env_step_limit_reached, _ = env.step(a)\n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "            if render:\n",
    "                print(\"\\n\" + line_break)\n",
    "                print(f\"Step {steps}\")\n",
    "                print(line_break)\n",
    "                print(f\"Action Performed = {env.action_space.get_action(a)}\")\n",
    "                env.render()\n",
    "                print(f\"Reward = {r}\")\n",
    "                print(f\"Done = {done}\")\n",
    "                print(f\"Step limit reached = {env_step_limit_reached}\")\n",
    "                input(\"Press enter to continue..\")\n",
    "\n",
    "                if done or env_step_limit_reached:\n",
    "                    print(\"\\n\" + line_break)\n",
    "                    print(\"EPISODE FINISHED\")\n",
    "                    print(line_break)\n",
    "                    print(f\"Goal reached = {env.goal_reached()}\")\n",
    "                    print(f\"Total steps = {steps}\")\n",
    "                    print(f\"Total reward = {episode_return}\")\n",
    "\n",
    "        env.render_mode = original_render_mode\n",
    "        return episode_return, steps, env.goal_reached()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"env_name\", type=str, help=\"benchmark scenario name\")\n",
    "    parser.add_argument(\"--render_eval\", action=\"store_true\",\n",
    "                        help=\"Renders final policy\")\n",
    "    parser.add_argument(\"-o\", \"--partially_obs\", action=\"store_true\",\n",
    "                        help=\"Partially Observable Mode\")\n",
    "    parser.add_argument(\"--hidden_sizes\", type=int, nargs=\"*\",\n",
    "                        default=[64, 64],\n",
    "                        help=\"(default=[64. 64])\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.001,\n",
    "                        help=\"Learning rate (default=0.001)\")\n",
    "    parser.add_argument(\"-t\", \"--training_steps\", type=int, default=20000,\n",
    "                        help=\"training steps (default=20000)\")\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=32,\n",
    "                        help=\"(default=32)\")\n",
    "    parser.add_argument(\"--target_update_freq\", type=int, default=1000,\n",
    "                        help=\"(default=1000)\")\n",
    "    parser.add_argument(\"--seed\", type=int, default=0,\n",
    "                        help=\"(default=0)\")\n",
    "    parser.add_argument(\"--replay_size\", type=int, default=100000,\n",
    "                        help=\"(default=100000)\")\n",
    "    parser.add_argument(\"--final_epsilon\", type=float, default=0.05,\n",
    "                        help=\"(default=0.05)\")\n",
    "    parser.add_argument(\"--init_epsilon\", type=float, default=1.0,\n",
    "                        help=\"(default=1.0)\")\n",
    "    parser.add_argument(\"--exploration_steps\", type=int, default=10000,\n",
    "                        help=\"(default=10000)\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "                        help=\"(default=0.99)\")\n",
    "    parser.add_argument(\"--quite\", action=\"store_false\",\n",
    "                        help=\"Run in Quite mode\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    env = nasim.make_benchmark(args.env_name,\n",
    "                               args.seed,\n",
    "                               fully_obs=not args.partially_obs,\n",
    "                               flat_actions=True,\n",
    "                               flat_obs=True)\n",
    "    dqn_agent = DQNAgent(env, verbose=args.quite, **vars(args))\n",
    "    dqn_agent.train()\n",
    "    dqn_agent.run_eval_episode(render=args.render_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8f38fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN with config:\n",
      "{'batch_size': 32,\n",
      " 'env': <nasim.envs.environment.NASimEnv object at 0x7f26cea3a200>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_sizes': [64, 64],\n",
      " 'kwargs': {'max_episodes': 1000},\n",
      " 'lr': 0.001,\n",
      " 'replay_size': 10000,\n",
      " 'seed': None,\n",
      " 'self': <__main__.DQNAgent object at 0x7f2668c1b490>,\n",
      " 'target_update_freq': 1000,\n",
      " 'training_steps': 200000,\n",
      " 'verbose': 1}\n",
      "\n",
      "Using Neural Network running on device=cuda:\n",
      "DQN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=207, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=64, out_features=72, bias=True)\n",
      ")\n",
      "\n",
      "Starting training\n",
      "\n",
      "Episode 10:\n",
      "\tsteps done = 7485 / 200000\n",
      "\treturn = -1108.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 20:\n",
      "\tsteps done = 17346 / 200000\n",
      "\treturn = -1669.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 30:\n",
      "\tsteps done = 27346 / 200000\n",
      "\treturn = -1041.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 40:\n",
      "\tsteps done = 36971 / 200000\n",
      "\treturn = -1023.0\n",
      "\tgoal = False\n",
      "\taverage = -1210.25\n",
      "\n",
      "Episode 50:\n",
      "\tsteps done = 46971 / 200000\n",
      "\treturn = -1034.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 60:\n",
      "\tsteps done = 56971 / 200000\n",
      "\treturn = -1013.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 70:\n",
      "\tsteps done = 66971 / 200000\n",
      "\treturn = -1009.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 80:\n",
      "\tsteps done = 76971 / 200000\n",
      "\treturn = -1069.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 90:\n",
      "\tsteps done = 86971 / 200000\n",
      "\treturn = -1014.0\n",
      "\tgoal = False\n",
      "\taverage = -1108.888888888889\n",
      "\n",
      "Episode 100:\n",
      "\tsteps done = 96971 / 200000\n",
      "\treturn = -911.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 110:\n",
      "\tsteps done = 106971 / 200000\n",
      "\treturn = -1065.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 120:\n",
      "\tsteps done = 116971 / 200000\n",
      "\treturn = -1017.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 130:\n",
      "\tsteps done = 126971 / 200000\n",
      "\treturn = -1020.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 140:\n",
      "\tsteps done = 136971 / 200000\n",
      "\treturn = -1017.0\n",
      "\tgoal = False\n",
      "\taverage = -1072.142857142857\n",
      "\n",
      "Episode 150:\n",
      "\tsteps done = 146971 / 200000\n",
      "\treturn = -1024.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 160:\n",
      "\tsteps done = 156971 / 200000\n",
      "\treturn = -1007.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 170:\n",
      "\tsteps done = 166971 / 200000\n",
      "\treturn = -1031.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 180:\n",
      "\tsteps done = 176971 / 200000\n",
      "\treturn = -1595.0\n",
      "\tgoal = False\n",
      "\n",
      "Episode 190:\n",
      "\tsteps done = 186971 / 200000\n",
      "\treturn = -1011.0\n",
      "\tgoal = False\n",
      "\taverage = -1088.3157894736842\n",
      "\n",
      "Episode 200:\n",
      "\tsteps done = 196971 / 200000\n",
      "\treturn = -1038.0\n",
      "\tgoal = False\n",
      "Training complete\n",
      "\n",
      "Episode 204:\n",
      "\tsteps done = 200000 / 200000\n",
      "\treturn = -29.0\n",
      "\tgoal = False\n",
      "[-1108.0, -1669.0, -1041.0, -1023.0, -1034.0, -1013.0, -1009.0, -1069.0, -1014.0, -911.0, -1065.0, -1017.0, -1020.0, -1017.0, -1024.0, -1007.0, -1031.0, -1595.0, -1011.0, -1038.0]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#NOW USING AGENT ABOVE INSTEAD OF IMPORTANT AGENT:\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#USED TO BE: from nasim.agents.ql_agent import TabularQLearningAgent\u001b[39;00m\n\u001b[1;32m      4\u001b[0m baseline_dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200000\u001b[39m, max_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mbaseline_dqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m baseline_dqn_agent\u001b[38;5;241m.\u001b[39mrun_eval_episode(render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[36], line 299\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mgoal = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgoal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mprint\u001b[39m(elems_to_avg)\n\u001b[0;32m--> 299\u001b[0m \u001b[43mplot_average_bar_chart\u001b[49m\u001b[43m(\u001b[49m\u001b[43melems_to_avg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[36], line 239\u001b[0m, in \u001b[0;36mDQNAgent.train.<locals>.plot_average_bar_chart\u001b[0;34m(avg_list)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_average_bar_chart\u001b[39m(avg_list):\n\u001b[1;32m    238\u001b[0m     x_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(avg_list) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#list of labels for x-axis\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mavg_list\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Creating bar chart\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStep Number\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    241\u001b[0m     plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReward\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, scaley\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[38;5;241m*\u001b[39margs, scalex\u001b[38;5;241m=\u001b[39mscalex, scaley\u001b[38;5;241m=\u001b[39mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:2309\u001b[0m, in \u001b[0;36mgca\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2307\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Figure\u001b[38;5;241m.\u001b[39mgca)\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgca\u001b[39m():\n\u001b[0;32m-> 2309\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgcf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgca()\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:906\u001b[0m, in \u001b[0;36mgcf\u001b[0;34m()\u001b[0m\n\u001b[1;32m    904\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[1;32m    905\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 906\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfigure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/_api/deprecation.py:454\u001b[0m, in \u001b[0;36mmake_keyword_only.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m name_idx:\n\u001b[1;32m    449\u001b[0m     warn_deprecated(\n\u001b[1;32m    450\u001b[0m         since, message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPassing the \u001b[39m\u001b[38;5;132;01m%(name)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%(obj_type)s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    451\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpositionally is deprecated since Matplotlib \u001b[39m\u001b[38;5;132;01m%(since)s\u001b[39;00m\u001b[38;5;124m; the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter will become keyword-only \u001b[39m\u001b[38;5;132;01m%(removal)s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    453\u001b[0m         name\u001b[38;5;241m=\u001b[39mname, obj_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:840\u001b[0m, in \u001b[0;36mfigure\u001b[0;34m(num, figsize, dpi, facecolor, edgecolor, frameon, FigureClass, clear, **kwargs)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(allnums) \u001b[38;5;241m==\u001b[39m max_open_warning \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    831\u001b[0m     _api\u001b[38;5;241m.\u001b[39mwarn_external(\n\u001b[1;32m    832\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMore than \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_open_warning\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m figures have been opened. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    833\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFigures created through the pyplot interface \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    837\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider using `matplotlib.pyplot.close()`.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    838\u001b[0m         \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m)\n\u001b[0;32m--> 840\u001b[0m manager \u001b[38;5;241m=\u001b[39m \u001b[43mnew_figure_manager\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdpi\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdpi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfacecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfacecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medgecolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43medgecolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframeon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframeon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m    \u001b[49m\u001b[43mFigureClass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFigureClass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m fig \u001b[38;5;241m=\u001b[39m manager\u001b[38;5;241m.\u001b[39mcanvas\u001b[38;5;241m.\u001b[39mfigure\n\u001b[1;32m    845\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fig_label:\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:383\u001b[0m, in \u001b[0;36mnew_figure_manager\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew_figure_manager\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    382\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a new figure manager instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 383\u001b[0m     \u001b[43m_warn_if_gui_out_of_main_thread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _get_backend_mod()\u001b[38;5;241m.\u001b[39mnew_figure_manager(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:361\u001b[0m, in \u001b[0;36m_warn_if_gui_out_of_main_thread\u001b[0;34m()\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_warn_if_gui_out_of_main_thread\u001b[39m():\n\u001b[1;32m    360\u001b[0m     warn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _get_required_interactive_framework(\u001b[43m_get_backend_mod\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(threading, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_native_id\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    363\u001b[0m             \u001b[38;5;66;03m# This compares native thread ids because even if Python-level\u001b[39;00m\n\u001b[1;32m    364\u001b[0m             \u001b[38;5;66;03m# Thread objects match, the underlying OS thread (which is what\u001b[39;00m\n\u001b[1;32m    365\u001b[0m             \u001b[38;5;66;03m# really matters) may be different on Python implementations with\u001b[39;00m\n\u001b[1;32m    366\u001b[0m             \u001b[38;5;66;03m# green threads.\u001b[39;00m\n\u001b[1;32m    367\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m threading\u001b[38;5;241m.\u001b[39mget_native_id() \u001b[38;5;241m!=\u001b[39m threading\u001b[38;5;241m.\u001b[39mmain_thread()\u001b[38;5;241m.\u001b[39mnative_id:\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:208\u001b[0m, in \u001b[0;36m_get_backend_mod\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mEnsure that a backend is selected and return it.\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03mThis is currently private, but may be made public in the future.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _backend_mod \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;66;03m# Use rcParams._get(\"backend\") to avoid going through the fallback\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# logic (which will (re)import pyplot and then call switch_backend if\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# we need to resolve the auto sentinel)\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[43mswitch_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrcParams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _backend_mod\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/matplotlib/pyplot.py:279\u001b[0m, in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    276\u001b[0m     current_framework \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39m_get_running_interactive_framework()\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (current_framework \u001b[38;5;129;01mand\u001b[39;00m required_framework\n\u001b[1;32m    278\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m current_framework \u001b[38;5;241m!=\u001b[39m required_framework):\n\u001b[0;32m--> 279\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    280\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load backend \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m which requires the \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m interactive \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframework, as \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m is currently running\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    282\u001b[0m                 newbackend, required_framework, current_framework))\n\u001b[1;32m    284\u001b[0m \u001b[38;5;66;03m# Load the new_figure_manager() and show() functions from the backend.\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# Classically, backends can directly export these functions.  This should\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# keep working for backcompat.\u001b[39;00m\n\u001b[1;32m    288\u001b[0m new_figure_manager \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(backend_mod, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_figure_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mImportError\u001b[0m: Cannot load backend 'TkAgg' which requires the 'tk' interactive framework, as 'headless' is currently running"
     ]
    }
   ],
   "source": [
    "#NOW USING AGENT ABOVE INSTEAD OF IMPORTANT AGENT:\n",
    "#USED TO BE: from nasim.agents.ql_agent import TabularQLearningAgent\n",
    "\n",
    "baseline_dqn_agent = DQNAgent(env, verbose=1, training_steps=200000, max_episodes=1000)\n",
    "baseline_dqn_agent.train()\n",
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb13cecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_dqn_agent = DQNAgent(env, verbose=1, training_steps=2000000) #10-20k steps, 10k episodes\n",
    "baseline_dqn_agent.train()\n",
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b4e4946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1019.0, 1000, False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_dqn_agent.run_eval_episode(render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6cc9d2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN with config:\n",
      "{'batch_size': 32,\n",
      " 'env': <nasim.envs.environment.NASimEnv object at 0x7fa97ec9dcf0>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_sizes': [64, 64],\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'replay_size': 10000,\n",
      " 'seed': None,\n",
      " 'self': <nasim.agents.dqn_agent.DQNAgent object at 0x7fa8b47a7be0>,\n",
      " 'target_update_freq': 1000,\n",
      " 'training_steps': 50000000,\n",
      " 'verbose': 1}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 127\u001b[0m\n\u001b[1;32m    124\u001b[0m env \u001b[38;5;241m=\u001b[39m nasim\u001b[38;5;241m.\u001b[39mmake_benchmark(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msmall\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;66;03m#Liam does it this way: env = nasim.load(\"small.yaml\")\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# Initializing and training agent\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m \u001b[43mDQNAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50000000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m dqn_agent\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/nasim/agents/dqn_agent.py:164\u001b[0m, in \u001b[0;36mDQNAgent.__init__\u001b[0;34m(self, env, seed, lr, training_steps, batch_size, replay_size, final_epsilon, exploration_steps, gamma, hidden_sizes, target_update_freq, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# Neural Network related attributes\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m                            \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()\n\u001b[1;32m    161\u001b[0m                            \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdqn \u001b[38;5;241m=\u001b[39m \u001b[43mDQN\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m               \u001b[49m\u001b[43mhidden_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m--> 164\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_actions\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose:\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mUsing Neural Network running on device=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/envs/nasim/lib/python3.10/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import necessary libraries, including which methods will be redefined\n",
    "import nasim\n",
    "import random\n",
    "from nasim.envs.action import Action\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "from nasim.envs.environment import NASimEnv\n",
    "\n",
    "# User-defined Python method to check whether the selected blocked_host is valid to select\n",
    "def check_host_valid(self, blocked_host):\n",
    "    if blocked_host == -1:\n",
    "        return\n",
    "    elif self.env.network.address_space[blocked_host] in self.env.network.get_sensitive_hosts():\n",
    "        raise SensitiveHostRemovalException\n",
    "    elif blocked_host == 0:\n",
    "        raise PublicHostRemovalException\n",
    "    else:\n",
    "        return\n",
    "\n",
    "# Setting the method\n",
    "DQNAgent.check_host_valid = check_host_valid\n",
    "    \n",
    "# Redefining the DQNAgent run_train_episode method\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False #Unnecessary now with loop below using steps < step_limit\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        max_host_index = len(self.env.network.host_num_map) - 1\n",
    "        \n",
    "        # Choosing random host index to be invalid... try/catch loop until valid host selected to block. Note: If -1, no host will be marked invalid\n",
    "        blocked_host = -1\n",
    "        if self.steps_done > 0:\n",
    "            while True:\n",
    "                try:0\n",
    "                    blocked_host = random.randint(-1,max_host_index)\n",
    "                    self.check_host_valid(blocked_host)\n",
    "                    break\n",
    "                except SensitiveHostRemovalException:\n",
    "                    pass\n",
    "                except PublicHostRemovalException:\n",
    "                    pass\n",
    "                \n",
    "        o, _ = self.env.reset()\n",
    "        \n",
    "        # If you wanted to see which host was blocked... used for the logging\n",
    "        print(\"Blocked host index:  \" + str(blocked_host))\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #steps < step_limit: #J: changed from env_step_limit_reached:     \n",
    "            #J: steps continuously updated at the bottom and will break as soon as step limit is reached\n",
    "            # Keep generating an action in the action space until it does not involve a blocked host\n",
    "            while True:\n",
    "                a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "                \n",
    "                if blocked_host == -1:\n",
    "                    break\n",
    "                else:\n",
    "                    action = self.env.action_space.get_action(a)\n",
    "                    target_host_index = self.env.network.host_num_map[action.target]\n",
    "                    if target_host_index != blocked_host:\n",
    "                        break\n",
    "                \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "# Setting the method\n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "# Training function... redefined because it wasn't converging originally\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    og_env = self.env\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        self.env = og_env\n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "# Set the method        \n",
    "DQNAgent.train = train\n",
    "\n",
    "# You can switch to a different benchmark if you want... like the scenario args posted or your own\n",
    "env = nasim.make_benchmark(\"small\")\n",
    "#Liam does it this way: env = nasim.load(\"small.yaml\")\n",
    "# Initializing and training agent\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=50000000)\n",
    "dqn_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cbb6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.run_eval_episode(render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648fa475",
   "metadata": {},
   "source": [
    "## Past Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662c71b",
   "metadata": {},
   "source": [
    "This was some code that didn't end up working if you wanted to see a previous attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadc01fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "capacity = 10\n",
    "s_dims = (5,)\n",
    "s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "#test_tuple.resize(test_tuple, [3,2])\n",
    "\n",
    "print(s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5db1431",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-e80566c61b5c>, line 58)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-e80566c61b5c>\"\u001b[0;36m, line \u001b[0;32m58\u001b[0m\n\u001b[0;31m    prev_num_actions = self.num_actions\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import nasim\n",
    "import random\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        \n",
    "        o = self.env.reset()\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #and steps < step_limit:\n",
    "            a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "        \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "    \n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    max_hosts = (self.env.scenario.get_description())['Hosts']\n",
    "    max_obs_dim = self.env.observation_space.shape\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        if self.steps_done > 0:\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.host_num_map)\n",
    "            print(self.env.network.subnets)\n",
    "            print(self.env.network.topology)\n",
    "            print(self.env.network.firewall)\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.address_space_bounds)\n",
    "            print(self.env.network.sensitive_addresses)\n",
    "            print(self.env.network.sensitive_hosts)\n",
    "\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = ReplayMemory(prev_replay_size,\n",
    "                                   #self.obs_dim,\n",
    "                                   #self.device)\n",
    "            \n",
    "            prev_observation_space = self.env.observation_space\n",
    "            prev_num_actions = self.num_actions\n",
    "            prev_obs_dim = self.obs_dim\n",
    "            prev_replay = self.replay\n",
    "            \n",
    "            scenario_args.update(num_hosts=random.randint(3,max_hosts))\n",
    "            \n",
    "            self.env =  nasim.generate(**scenario_args)\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = prev_replay\n",
    "            \n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "            \n",
    "DQNAgent.train = train\n",
    "\n",
    "print(scenario_args)\n",
    "env = nasim.generate(**scenario_args)\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=100000)\n",
    "dqn_agent.train()\n",
    "dqn_agent.run_eval_episode(render=args.render_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776e4f12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nasim",
   "language": "python",
   "name": "nasim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
