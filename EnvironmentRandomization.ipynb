{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20f1931",
   "metadata": {},
   "source": [
    "## Initial Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38702355",
   "metadata": {},
   "source": [
    "This is all some intro code just to visualize some of the settings and get a baseline. You can run through it if you want to get familiar with what the scenario arguments, agents, and training looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5756d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = 'data.yaml'\n",
    "\n",
    "def writeToYAML():\n",
    "    with open(testfile, 'w') as f:\n",
    "        data = yaml.dump(curr_data, f, sort_keys=False, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_dict(dict_obj, indent = 0):\n",
    "    ''' Pretty Print nested dictionary with given indent level  \n",
    "    '''\n",
    "    # Iterate over all key-value pairs of dictionary\n",
    "    for key, value in dict_obj.items():\n",
    "        # If value is dict type, then print nested dict \n",
    "        if isinstance(value, dict):\n",
    "            print(' ' * indent, key, ':', '{')\n",
    "            print_nested_dict(value, indent + 4)\n",
    "            print(' ' * indent, '}')\n",
    "        else:\n",
    "            print(' ' * indent, key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89b2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_args={\n",
    "    \"num_hosts\": 5,\n",
    "    \"num_services\": 3,\n",
    "    \"num_os\": 2,\n",
    "    \"num_processes\": 2,\n",
    "    \"num_exploits\": None,\n",
    "    \"num_privescs\": None,\n",
    "    \"r_sensitive\": 10,\n",
    "    \"r_user\": 10,\n",
    "    \"exploit_cost\": 1,\n",
    "    \"exploit_probs\": 1.0,\n",
    "    \"privesc_cost\": 1,\n",
    "    \"privesc_probs\": 1.0,\n",
    "    \"service_scan_cost\": 1,\n",
    "    \"os_scan_cost\": 1,\n",
    "    \"subnet_scan_cost\": 1,\n",
    "    \"process_scan_cost\": 1,\n",
    "    \"uniform\": False,\n",
    "    \"alpha_H\": 2.0,\n",
    "    \"alpha_V\": 2.0,\n",
    "    \"lambda_V\": 1.0,\n",
    "    \"restrictiveness\": 5,\n",
    "    \"random_goal\": False,\n",
    "    \"base_host_value\": 1,\n",
    "    \"host_discovery_value\": 1,\n",
    "    \"seed\": None,\n",
    "    \"name\": None,\n",
    "    \"step_limit\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b27fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasim\n",
    "import json\n",
    "env = nasim.generate(**scenario_args)\n",
    "env = nasim.make_benchmark(\"huge-gen\")\n",
    "env = nasim.load(\"unreachable.yaml\")\n",
    "\n",
    "scenario_desc = env.scenario.get_description()\n",
    "scenario_dict = env.scenario.scenario_dict\n",
    "print_nested_dict(scenario_desc,4)\n",
    "print_nested_dict(scenario_dict,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1f15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.get_minimum_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a0d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nasim.agents.ql_agent import TabularQLearningAgent\n",
    "\n",
    "ql_agent = TabularQLearningAgent(env, verbose=1, training_steps=50000)\n",
    "ql_agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b023e0",
   "metadata": {},
   "source": [
    "## Current Code \n",
    "Here is the main code to test/run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c4bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial scenario arguments... we will be editing the number of hosts by marking actions involving them as invalid\n",
    "scenario_args={\n",
    "    \"num_hosts\": 5,\n",
    "    \"num_services\": 3,\n",
    "    \"num_os\": 2,\n",
    "    \"num_processes\": 2,\n",
    "    \"num_exploits\": None,\n",
    "    \"num_privescs\": None,\n",
    "    \"r_sensitive\": 10,\n",
    "    \"r_user\": 10,\n",
    "    \"exploit_cost\": 1,\n",
    "    \"exploit_probs\": 1.0,\n",
    "    \"privesc_cost\": 1,\n",
    "    \"privesc_probs\": 1.0,\n",
    "    \"service_scan_cost\": 1,\n",
    "    \"os_scan_cost\": 1,\n",
    "    \"subnet_scan_cost\": 1,\n",
    "    \"process_scan_cost\": 1,\n",
    "    \"uniform\": False,\n",
    "    \"alpha_H\": 2.0,\n",
    "    \"alpha_V\": 2.0,\n",
    "    \"lambda_V\": 1.0,\n",
    "    \"restrictiveness\": 5,\n",
    "    \"random_goal\": False,\n",
    "    \"base_host_value\": 1,\n",
    "    \"host_discovery_value\": 1,\n",
    "    \"seed\": None,\n",
    "    \"name\": None,\n",
    "    \"step_limit\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbd02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Python user-defined exceptions\n",
    "class SensitiveHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (sensitive host needs to remain in network)\"\n",
    "    pass\n",
    "\n",
    "class PublicHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (public host to enter the network... specific to this configuration)\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99801e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries, including which methods will be redefined\n",
    "import nasim\n",
    "import random\n",
    "from nasim.envs.action import Action\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "from nasim.envs.environment import NASimEnv\n",
    "\n",
    "# User-defined Python method to check whether the selected blocked_host is valid to select\n",
    "def check_host_valid(self, blocked_host):\n",
    "    if blocked_host == -1:\n",
    "        return\n",
    "    elif self.env.network.address_space[blocked_host] in self.env.network.get_sensitive_hosts():\n",
    "        raise SensitiveHostRemovalException\n",
    "    elif blocked_host == 0:\n",
    "        raise PublicHostRemovalException\n",
    "    else:\n",
    "        return\n",
    "# Setting the method\n",
    "DQNAgent.check_host_valid = check_host_valid\n",
    "    \n",
    "# Redefining the DQNAgent run_train_episode method\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        max_host_index = len(self.env.network.host_num_map) - 1\n",
    "        \n",
    "        # Choosing random host index to be invalid... try/catch loop until valid host selected to block. Note: If -1, no host will be marked invalid\n",
    "        blocked_host = -1\n",
    "        if self.steps_done > 0:\n",
    "            while True:\n",
    "                try:\n",
    "                    blocked_host = random.randint(-1,max_host_index)\n",
    "                    self.check_host_valid(blocked_host)\n",
    "                    break\n",
    "                except SensitiveHostRemovalException:\n",
    "                    pass\n",
    "                except PublicHostRemovalException:\n",
    "                    pass\n",
    "                \n",
    "        o = self.env.reset()\n",
    "        \n",
    "        # If you wanted to see which host was blocked... used for the logging\n",
    "        print(\"Blocked host index:  \" + str(blocked_host))\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #and steps < step_limit:\n",
    "            # Keep generating an action in the action space until it does not involve a blocked host\n",
    "            while True:\n",
    "                a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "                \n",
    "                if blocked_host == -1:\n",
    "                    break\n",
    "                else:\n",
    "                    action = self.env.action_space.get_action(a)\n",
    "                    target_host_index = self.env.network.host_num_map[action.target]\n",
    "                    if target_host_index != blocked_host:\n",
    "                        break\n",
    "                \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "# Setting the method\n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "# Training function... redefined because it wasn't converging originally\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    og_env = self.env\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        self.env = og_env\n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "# Set the method        \n",
    "DQNAgent.train = train\n",
    "\n",
    "# You can switch to a different benchmark if you want... like the scenario args posted or your own\n",
    "env = nasim.make_benchmark(\"small\")\n",
    "# Initializing and training agent\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=50000000)\n",
    "dqn_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff038d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_agent.run_eval_episode(render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a2bda",
   "metadata": {},
   "source": [
    "## Past Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fb427",
   "metadata": {},
   "source": [
    "This was some code that didn't end up working if you wanted to see a previous attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "capacity = 10\n",
    "s_dims = (5,)\n",
    "s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "#test_tuple.resize(test_tuple, [3,2])\n",
    "\n",
    "print(s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06785b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasim\n",
    "import random\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        \n",
    "        o = self.env.reset()\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #and steps < step_limit:\n",
    "            a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "        \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "    \n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    max_hosts = (self.env.scenario.get_description())['Hosts']\n",
    "    max_obs_dim = self.env.observation_space.shape\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        if self.steps_done > 0:\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.host_num_map)\n",
    "            print(self.env.network.subnets)\n",
    "            print(self.env.network.topology)\n",
    "            print(self.env.network.firewall)\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.address_space_bounds)\n",
    "            print(self.env.network.sensitive_addresses)\n",
    "            print(self.env.network.sensitive_hosts)\n",
    "\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = ReplayMemory(prev_replay_size,\n",
    "                                   #self.obs_dim,\n",
    "                                   #self.device)\n",
    "            \n",
    "            prev_observation_space = self.env.observation_space\n",
    "            prev_num_actions = self.num_actions\n",
    "            prev_obs_dim = self.obs_dim\n",
    "            prev_replay = self.replay\n",
    "            \n",
    "            scenario_args.update(num_hosts=random.randint(3,max_hosts))\n",
    "            \n",
    "            self.env =  nasim.generate(**scenario_args)\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = prev_replay\n",
    "            \n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "            \n",
    "DQNAgent.train = train\n",
    "\n",
    "print(scenario_args)\n",
    "env = nasim.generate(**scenario_args)\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=100000)\n",
    "dqn_agent.train()\n",
    "dqn_agent.run_eval_episode(render=args.render_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
