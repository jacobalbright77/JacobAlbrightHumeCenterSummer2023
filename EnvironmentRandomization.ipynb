{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f20f1931",
   "metadata": {},
   "source": [
    "## Initial Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38702355",
   "metadata": {},
   "source": [
    "This is all some intro code just to visualize some of the settings and get a baseline. You can run through it if you want to get familiar with what the scenario arguments, agents, and training looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5756d468",
   "metadata": {},
   "outputs": [],
   "source": [
    "testfile = 'data.yaml'\n",
    "\n",
    "\n",
    "def writeToYAML():\n",
    "    with open(testfile, 'w') as f:\n",
    "        data = yaml.dump(curr_data, f, sort_keys=False, default_flow_style=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111fbc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nested_dict(dict_obj, indent = 0):\n",
    "    ''' Pretty Print nested dictionary with given indent level  \n",
    "    '''\n",
    "    # Iterate over all key-value pairs of dictionary\n",
    "    for key, value in dict_obj.items():\n",
    "        # If value is dict type, then print nested dict \n",
    "        if isinstance(value, dict):\n",
    "            print(' ' * indent, key, ':', '{')\n",
    "            print_nested_dict(value, indent + 4)\n",
    "            print(' ' * indent, '}')\n",
    "        else:\n",
    "            print(' ' * indent, key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a89b2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario_args={\n",
    "    \"num_hosts\": 5,         # Number of hosts in the network \n",
    "    \"num_services\": 3,      # Number of services on the network (ssh, ftp, http)\n",
    "    \"num_os\": 2,            # Number of operatings systems on the network (windows, linux, etc)\n",
    "    \"num_processes\": 2,     # Number of processes on the network (tomcat, daclsvc, etc)\n",
    "    \"num_exploits\": None,   # \n",
    "    \"num_privescs\": None,\n",
    "    \"r_sensitive\": 10,\n",
    "    \"r_user\": 10,\n",
    "    \"exploit_cost\": 1,\n",
    "    \"exploit_probs\": 1.0,\n",
    "    \"privesc_cost\": 1,\n",
    "    \"privesc_probs\": 1.0,\n",
    "    \"service_scan_cost\": 1,\n",
    "    \"os_scan_cost\": 1,\n",
    "    \"subnet_scan_cost\": 1,\n",
    "    \"process_scan_cost\": 1,\n",
    "    \"uniform\": False,\n",
    "    \"alpha_H\": 2.0,\n",
    "    \"alpha_V\": 2.0,\n",
    "    \"lambda_V\": 1.0,\n",
    "    \"restrictiveness\": 5,\n",
    "    \"random_goal\": False,\n",
    "    \"base_host_value\": 1,\n",
    "    \"host_discovery_value\": 1,\n",
    "    \"seed\": None,\n",
    "    \"name\": None,\n",
    "    \"step_limit\": None}\n",
    "\n",
    "#Scenario Generator Parameter List: https://networkattacksimulator.readthedocs.io/en/latest/reference/scenarios/generator.html#scenario-generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5b27fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name : unreachable\n",
      "     Type : static\n",
      "     Subnets : 4\n",
      "     Hosts : 3\n",
      "     OS : 1\n",
      "     Services : 1\n",
      "     Processes : 1\n",
      "     Exploits : 1\n",
      "     PrivEscs : 1\n",
      "     Actions : 18\n",
      "     Observation Dims : (4, 14)\n",
      "     States : 576\n",
      "     Step Limit : 1000\n",
      "       subnets : [1, 1, 1, 1]\n",
      "       topology : [[1, 1, 0, 0], [1, 1, 1, 1], [0, 1, 1, 1], [0, 1, 1, 1]]\n",
      "       os : ['linux']\n",
      "       services : ['ssh']\n",
      "       processes : ['tomcat']\n",
      "       sensitive_hosts : {\n",
      "           (2, 0) : 100\n",
      "           (3, 0) : 100\n",
      "       }\n",
      "       exploits : {\n",
      "           e_ssh : {\n",
      "               service : ssh\n",
      "               os : linux\n",
      "               prob : 0.8\n",
      "               cost : 1\n",
      "               access : 1\n",
      "           }\n",
      "       }\n",
      "       privilege_escalation : {\n",
      "           pe_tomcat : {\n",
      "               process : tomcat\n",
      "               os : linux\n",
      "               prob : 1.0\n",
      "               cost : 1\n",
      "               access : 2\n",
      "           }\n",
      "       }\n",
      "       os_scan_cost : 1\n",
      "       service_scan_cost : 1\n",
      "       subnet_scan_cost : 1\n",
      "       process_scan_cost : 1\n",
      "       firewall : {\n",
      "           (0, 1) : []\n",
      "           (1, 0) : []\n",
      "           (1, 2) : []\n",
      "           (2, 1) : []\n",
      "           (1, 3) : []\n",
      "           (3, 1) : []\n",
      "           (2, 3) : []\n",
      "           (3, 2) : []\n",
      "       }\n",
      "       host : {\n",
      "           (1, 0) : Host: {\n",
      "\taddress: (1, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 0.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: True\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t\t(3, 0): ['ssh']\n",
      "\t}\n",
      "           (2, 0) : Host: {\n",
      "\taddress: (2, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 100.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: True\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t\t(1, 0): ['ssh']\n",
      "\t}\n",
      "           (3, 0) : Host: {\n",
      "\taddress: (3, 0)\n",
      "\tcompromised: False\n",
      "\treachable: False\n",
      "\tvalue: 100.0\n",
      "\taccess: 0\n",
      "\tOS: {\n",
      "\t\tlinux: True\n",
      "\t}\n",
      "\tservices: {\n",
      "\t\tssh: True\n",
      "\t}\n",
      "\tprocesses: {\n",
      "\t\ttomcat: True\n",
      "\t}\n",
      "\tfirewall: {\n",
      "\t}\n",
      "       }\n",
      "       step_limit : 1000\n"
     ]
    }
   ],
   "source": [
    "import nasim\n",
    "import json\n",
    "env = nasim.generate(**scenario_args)\n",
    "env = nasim.make_benchmark(\"huge-gen\")\n",
    "env = nasim.load(\"unreachable.yaml\")\n",
    "\n",
    "scenario_desc = env.scenario.get_description()\n",
    "scenario_dict = env.scenario.scenario_dict\n",
    "print_nested_dict(scenario_desc,4)\n",
    "print_nested_dict(scenario_dict,6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0b1f15bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.get_minimum_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "991a0d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Tabular Q-Learning with config:\n",
      "{'env': <nasim.envs.environment.NASimEnv object at 0x11ebe0b20>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'seed': None,\n",
      " 'self': <nasim.agents.ql_agent.TabularQLearningAgent object at 0x11eb01210>,\n",
      " 'training_steps': 50000,\n",
      " 'verbose': 1}\n",
      "\n",
      "Starting training\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnasim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mql_agent\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TabularQLearningAgent\n\u001b[1;32m      3\u001b[0m ql_agent \u001b[38;5;241m=\u001b[39m TabularQLearningAgent(env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43mql_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py:166\u001b[0m, in \u001b[0;36mTabularQLearningAgent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=162'>163</a>\u001b[0m training_steps_remaining \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_steps\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=164'>165</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_done \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_steps:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=165'>166</a>\u001b[0m     ep_results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_train_episode(training_steps_remaining)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=166'>167</a>\u001b[0m     ep_return, ep_steps, goal \u001b[39m=\u001b[39m ep_results\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=167'>168</a>\u001b[0m     num_episodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py:213\u001b[0m, in \u001b[0;36mTabularQLearningAgent.run_train_episode\u001b[0;34m(self, step_limit)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=210'>211</a>\u001b[0m next_s, r, done, env_step_limit_reached, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(a)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=211'>212</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_done \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=212'>213</a>\u001b[0m td_error, s_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimize(s, a, next_s, r, done)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=213'>214</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mtd_error\u001b[39m\u001b[39m\"\u001b[39m, td_error, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_done)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=214'>215</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39ms_value\u001b[39m\u001b[39m\"\u001b[39m, s_value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps_done)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py:141\u001b[0m, in \u001b[0;36mTabularQLearningAgent.optimize\u001b[0;34m(self, s, a, next_s, r, done)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\u001b[39mself\u001b[39m, s, a, next_s, r, done):\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=139'>140</a>\u001b[0m     \u001b[39m# get q_val for state and action performed in that state\u001b[39;00m\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=140'>141</a>\u001b[0m     q_vals_raw \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqfunc\u001b[39m.\u001b[39;49mforward(s)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=141'>142</a>\u001b[0m     q_val \u001b[39m=\u001b[39m q_vals_raw[a]\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=143'>144</a>\u001b[0m     \u001b[39m# get target q val = max val of next state\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py:56\u001b[0m, in \u001b[0;36mTabularQFunction.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=53'>54</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=54'>55</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(x, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=55'>56</a>\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(x\u001b[39m.\u001b[39mastype(np\u001b[39m.\u001b[39;49mint))\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=56'>57</a>\u001b[0m     \u001b[39mif\u001b[39;00m x \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_func:\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/ql_agent.py?line=57'>58</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mq_func[x] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_actions, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mfloat32)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py:305\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=299'>300</a>\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=300'>301</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIn the future `np.\u001b[39m\u001b[39m{\u001b[39;00mattr\u001b[39m}\u001b[39;00m\u001b[39m` will be defined as the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=301'>302</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcorresponding NumPy scalar.\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFutureWarning\u001b[39;00m, stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=303'>304</a>\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39min\u001b[39;00m __former_attrs__:\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=304'>305</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(__former_attrs__[attr])\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=306'>307</a>\u001b[0m \u001b[39m# Importing Tester requires importing all of UnitTest which is not a\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=307'>308</a>\u001b[0m \u001b[39m# cheap import Since it is mainly used in test suits, we lazy import it\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=308'>309</a>\u001b[0m \u001b[39m# here to save on the order of 10 ms of import time for most users\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=309'>310</a>\u001b[0m \u001b[39m#\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=310'>311</a>\u001b[0m \u001b[39m# The previous way Tester was imported also had a side effect of adding\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=311'>312</a>\u001b[0m \u001b[39m# the full `numpy.testing` namespace\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/__init__.py?line=312'>313</a>\u001b[0m \u001b[39mif\u001b[39;00m attr \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtesting\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
     ]
    }
   ],
   "source": [
    "from nasim.agents.ql_agent import TabularQLearningAgent\n",
    "\n",
    "ql_agent = TabularQLearningAgent(env, verbose=1, training_steps=50000)\n",
    "ql_agent.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b023e0",
   "metadata": {},
   "source": [
    "## Current Code \n",
    "Here is the main code to test/run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73c4bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial scenario arguments... we will be editing the number of hosts by marking actions involving them as invalid\n",
    "scenario_args={\n",
    "    \"num_hosts\": 5,\n",
    "    \"num_services\": 3,\n",
    "    \"num_os\": 2,\n",
    "    \"num_processes\": 2,\n",
    "    \"num_exploits\": None,\n",
    "    \"num_privescs\": None,\n",
    "    \"r_sensitive\": 10,\n",
    "    \"r_user\": 10,\n",
    "    \"exploit_cost\": 1,\n",
    "    \"exploit_probs\": 1.0,\n",
    "    \"privesc_cost\": 1,\n",
    "    \"privesc_probs\": 1.0,\n",
    "    \"service_scan_cost\": 1,\n",
    "    \"os_scan_cost\": 1,\n",
    "    \"subnet_scan_cost\": 1,\n",
    "    \"process_scan_cost\": 1,\n",
    "    \"uniform\": False,\n",
    "    \"alpha_H\": 2.0,\n",
    "    \"alpha_V\": 2.0,\n",
    "    \"lambda_V\": 1.0,\n",
    "    \"restrictiveness\": 5,\n",
    "    \"random_goal\": False,\n",
    "    \"base_host_value\": 1,\n",
    "    \"host_discovery_value\": 1,\n",
    "    \"seed\": None,\n",
    "    \"name\": None,\n",
    "    \"step_limit\": None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bbd02bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Python user-defined exceptions\n",
    "class SensitiveHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (sensitive host needs to remain in network)\"\n",
    "    pass\n",
    "\n",
    "class PublicHostRemovalException(Exception):\n",
    "    \"Raised when selected network host cannot be removed (public host to enter the network... specific to this configuration)\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c99801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running DQN with config:\n",
      "{'batch_size': 32,\n",
      " 'env': <nasim.envs.environment.NASimEnv object at 0x112635ae0>,\n",
      " 'exploration_steps': 10000,\n",
      " 'final_epsilon': 0.05,\n",
      " 'gamma': 0.99,\n",
      " 'hidden_sizes': [64, 64],\n",
      " 'kwargs': {},\n",
      " 'lr': 0.001,\n",
      " 'replay_size': 10000,\n",
      " 'seed': None,\n",
      " 'self': <nasim.agents.dqn_agent.DQNAgent object at 0x12849c5e0>,\n",
      " 'target_update_freq': 1000,\n",
      " 'training_steps': 50000000,\n",
      " 'verbose': 1}\n",
      "\n",
      "Using Neural Network running on device=cpu:\n",
      "DQN(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=207, out_features=64, bias=True)\n",
      "    (1): Linear(in_features=64, out_features=64, bias=True)\n",
      "  )\n",
      "  (out): Linear(in_features=64, out_features=72, bias=True)\n",
      ")\n",
      "\n",
      "Starting training\n",
      "Blocked host index:  -1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Initializing and training agent\u001b[39;00m\n\u001b[1;32m    124\u001b[0m dqn_agent \u001b[38;5;241m=\u001b[39m DQNAgent(env, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, training_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000000\u001b[39m)\n\u001b[0;32m--> 125\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [8], line 85\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_steps:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m og_env\n\u001b[0;32m---> 85\u001b[0m     ep_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_train_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_steps_remaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m     ep_return, ep_steps, goal \u001b[38;5;241m=\u001b[39m ep_results\n\u001b[1;32m     87\u001b[0m     num_episodes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn [8], line 61\u001b[0m, in \u001b[0;36mrun_train_episode\u001b[0;34m(self, step_limit)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     60\u001b[0m next_o, r, done, env_step_limit_reached, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[0;32m---> 61\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     63\u001b[0m loss, mean_v \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimize()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py:60\u001b[0m, in \u001b[0;36mReplayMemory.store\u001b[0;34m(self, s, a, next_s, r, done)\u001b[0m\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstore\u001b[39m(\u001b[39mself\u001b[39m, s, a, next_s, r, done):\n\u001b[0;32m---> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=59'>60</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ms_buf[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mptr] \u001b[39m=\u001b[39m s\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=60'>61</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma_buf[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptr] \u001b[39m=\u001b[39m a\n\u001b[1;32m     <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=61'>62</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnext_s_buf[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mptr] \u001b[39m=\u001b[39m next_s\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array would exceed the maximum number of dimension of 1."
     ]
    }
   ],
   "source": [
    "# Import necessary libraries, including which methods will be redefined\n",
    "import nasim\n",
    "import random\n",
    "from nasim.envs.action import Action\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "from nasim.envs.environment import NASimEnv\n",
    "\n",
    "# User-defined Python method to check whether the selected blocked_host is valid to select\n",
    "def check_host_valid(self, blocked_host):\n",
    "    if blocked_host == -1:\n",
    "        return\n",
    "    elif self.env.network.address_space[blocked_host] in self.env.network.get_sensitive_hosts():\n",
    "        raise SensitiveHostRemovalException\n",
    "    elif blocked_host == 0:\n",
    "        raise PublicHostRemovalException\n",
    "    else:\n",
    "        return\n",
    "# Setting the method\n",
    "DQNAgent.check_host_valid = check_host_valid\n",
    "    \n",
    "# Redefining the DQNAgent run_train_episode method\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        max_host_index = len(self.env.network.host_num_map) - 1\n",
    "        \n",
    "        # Choosing random host index to be invalid... try/catch loop until valid host selected to block. Note: If -1, no host will be marked invalid\n",
    "        blocked_host = -1\n",
    "        if self.steps_done > 0:\n",
    "            while True:\n",
    "                try:\n",
    "                    blocked_host = random.randint(-1,max_host_index)\n",
    "                    self.check_host_valid(blocked_host)\n",
    "                    break\n",
    "                except SensitiveHostRemovalException:\n",
    "                    pass\n",
    "                except PublicHostRemovalException:\n",
    "                    pass\n",
    "                \n",
    "        o = self.env.reset()\n",
    "        \n",
    "        # If you wanted to see which host was blocked... used for the logging\n",
    "        print(\"Blocked host index:  \" + str(blocked_host))\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #and steps < step_limit:\n",
    "            # Keep generating an action in the action space until it does not involve a blocked host\n",
    "            while True:\n",
    "                a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "                \n",
    "                if blocked_host == -1:\n",
    "                    break\n",
    "                else:\n",
    "                    action = self.env.action_space.get_action(a)\n",
    "                    target_host_index = self.env.network.host_num_map[action.target]\n",
    "                    if target_host_index != blocked_host:\n",
    "                        break\n",
    "                \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "\n",
    "# Setting the method\n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "# Training function... redefined because it wasn't converging originally\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    og_env = self.env\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        self.env = og_env\n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "# Set the method        \n",
    "DQNAgent.train = train\n",
    "\n",
    "# You can switch to a different benchmark if you want... like the scenario args posted or your own\n",
    "env = nasim.make_benchmark(\"small\")\n",
    "# Initializing and training agent\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=50000000)\n",
    "dqn_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff038d0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (14,) into shape (23,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdqn_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_eval_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py:305\u001b[0m, in \u001b[0;36mDQNAgent.run_eval_episode\u001b[0;34m(self, env, render, eval_epsilon, render_mode)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=301'>302</a>\u001b[0m original_render_mode \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mrender_mode\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=302'>303</a>\u001b[0m env\u001b[39m.\u001b[39mrender_mode \u001b[39m=\u001b[39m render_mode\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=304'>305</a>\u001b[0m o, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mreset()\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=305'>306</a>\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/agents/dqn_agent.py?line=306'>307</a>\u001b[0m env_step_limit_reached \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py:132\u001b[0m, in \u001b[0;36mNASimEnv.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py?line=129'>130</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py?line=130'>131</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnetwork\u001b[39m.\u001b[39mreset(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_state)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py?line=131'>132</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcurrent_state\u001b[39m.\u001b[39;49mget_initial_observation(\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py?line=132'>133</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfully_obs\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py?line=133'>134</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py?line=135'>136</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflat_obs:\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/environment.py?line=136'>137</a>\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlast_obs\u001b[39m.\u001b[39mnumpy_flat()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/state.py:120\u001b[0m, in \u001b[0;36mState.get_initial_observation\u001b[0;34m(self, fully_obs)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/state.py?line=115'>116</a>\u001b[0m     host_obs \u001b[39m=\u001b[39m host\u001b[39m.\u001b[39mobserve(address\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/state.py?line=116'>117</a>\u001b[0m                             reachable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/state.py?line=117'>118</a>\u001b[0m                             discovered\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/state.py?line=118'>119</a>\u001b[0m     host_idx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_host_idx(host_addr)\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/state.py?line=119'>120</a>\u001b[0m     obs\u001b[39m.\u001b[39;49mupdate_from_host(host_idx, host_obs)\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/state.py?line=120'>121</a>\u001b[0m \u001b[39mreturn\u001b[39;00m obs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/observation.py:107\u001b[0m, in \u001b[0;36mObservation.update_from_host\u001b[0;34m(self, host_idx, host_obs_vector)\u001b[0m\n\u001b[1;32m    <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/observation.py?line=105'>106</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate_from_host\u001b[39m(\u001b[39mself\u001b[39m, host_idx, host_obs_vector):\n\u001b[0;32m--> <a href='file:///Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/nasim/envs/observation.py?line=106'>107</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtensor[host_idx][:] \u001b[39m=\u001b[39m host_obs_vector\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (14,) into shape (23,)"
     ]
    }
   ],
   "source": [
    "dqn_agent.run_eval_episode(render=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3a2bda",
   "metadata": {},
   "source": [
    "## Past Attempts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2fb427",
   "metadata": {},
   "source": [
    "This was some code that didn't end up working if you wanted to see a previous attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6df1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "capacity = 10\n",
    "s_dims = (5,)\n",
    "s_buf = np.zeros((capacity, *s_dims), dtype=np.float32)\n",
    "#test_tuple.resize(test_tuple, [3,2])\n",
    "\n",
    "print(s_buf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06785b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nasim\n",
    "import random\n",
    "from nasim.agents.dqn_agent import DQNAgent\n",
    "\n",
    "def run_train_episode(self, step_limit):\n",
    "        done = False\n",
    "        env_step_limit_reached = False\n",
    "        steps = 0\n",
    "        episode_return = 0\n",
    "        \n",
    "        o = self.env.reset()\n",
    "        \n",
    "        while not done and not env_step_limit_reached: #and steps < step_limit:\n",
    "            a = self.get_egreedy_action(o, self.get_epsilon())\n",
    "        \n",
    "            next_o, r, done, env_step_limit_reached, _ = self.env.step(a)\n",
    "            self.replay.store(o, a, next_o, r, done)\n",
    "            self.steps_done += 1\n",
    "            loss, mean_v = self.optimize()\n",
    "            \n",
    "            o = next_o\n",
    "            episode_return += r\n",
    "            steps += 1\n",
    "\n",
    "        return episode_return, steps, self.env.goal_reached()\n",
    "    \n",
    "DQNAgent.run_train_episode = run_train_episode\n",
    "\n",
    "def train(self):\n",
    "    if self.verbose:\n",
    "        print(\"\\nStarting training\")\n",
    "\n",
    "    num_episodes = 0\n",
    "    training_steps_remaining = self.training_steps\n",
    "    max_hosts = (self.env.scenario.get_description())['Hosts']\n",
    "    max_obs_dim = self.env.observation_space.shape\n",
    "    \n",
    "    while self.steps_done < self.training_steps:\n",
    "        if self.steps_done > 0:\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.host_num_map)\n",
    "            print(self.env.network.subnets)\n",
    "            print(self.env.network.topology)\n",
    "            print(self.env.network.firewall)\n",
    "            print(self.env.network.address_space)\n",
    "            print(self.env.network.address_space_bounds)\n",
    "            print(self.env.network.sensitive_addresses)\n",
    "            print(self.env.network.sensitive_hosts)\n",
    "\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = ReplayMemory(prev_replay_size,\n",
    "                                   #self.obs_dim,\n",
    "                                   #self.device)\n",
    "            \n",
    "            prev_observation_space = self.env.observation_space\n",
    "            prev_num_actions = self.num_actions\n",
    "            prev_obs_dim = self.obs_dim\n",
    "            prev_replay = self.replay\n",
    "            \n",
    "            scenario_args.update(num_hosts=random.randint(3,max_hosts))\n",
    "            \n",
    "            self.env =  nasim.generate(**scenario_args)\n",
    "            self.env.observation_space = prev_observation_space\n",
    "            self.num_actions = prev_num_actions\n",
    "            self.obs_dim = prev_obs_dim\n",
    "            self.replay = prev_replay\n",
    "            \n",
    "        ep_results = self.run_train_episode(training_steps_remaining)\n",
    "        ep_return, ep_steps, goal = ep_results\n",
    "        num_episodes += 1\n",
    "        training_steps_remaining -= ep_steps\n",
    "\n",
    "        self.logger.add_scalar(\"episode\", num_episodes, self.steps_done)\n",
    "        self.logger.add_scalar(\n",
    "            \"epsilon\", self.get_epsilon(), self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_return\", ep_return, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_steps\", ep_steps, self.steps_done\n",
    "        )\n",
    "        self.logger.add_scalar(\n",
    "            \"episode_goal_reached\", int(goal), self.steps_done\n",
    "        )\n",
    "\n",
    "        if num_episodes % 10 == 0 and self.verbose:\n",
    "            print(f\"\\nEpisode {num_episodes}:\")\n",
    "            print(f\"\\tsteps done = {self.steps_done} / \"\n",
    "                    f\"{self.training_steps}\")\n",
    "            print(f\"\\treturn = {ep_return}\")\n",
    "            print(f\"\\tgoal = {goal}\")\n",
    "\n",
    "    self.logger.close()\n",
    "    if self.verbose:\n",
    "        print(\"Training complete\")\n",
    "        print(f\"\\nEpisode {num_episodes}:\")\n",
    "        print(f\"\\tsteps done = {self.steps_done} / {self.training_steps}\")\n",
    "        print(f\"\\treturn = {ep_return}\")\n",
    "        print(f\"\\tgoal = {goal}\")\n",
    "            \n",
    "DQNAgent.train = train\n",
    "\n",
    "print(scenario_args)\n",
    "env = nasim.generate(**scenario_args)\n",
    "dqn_agent = DQNAgent(env, verbose=1, training_steps=100000)\n",
    "dqn_agent.train()\n",
    "dqn_agent.run_eval_episode(render=args.render_eval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
